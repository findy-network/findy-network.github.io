<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Findy Agency – Findy Agency</title><link>/</link><description>Recent content on Findy Agency</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Thu, 07 Apr 2022 00:00:00 +0000</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml"/><item><title>Blog: SSI-Empowered Identity Provider</title><link>/blog/2022/04/07/ssi-empowered-identity-provider/</link><pubDate>Thu, 07 Apr 2022 00:00:00 +0000</pubDate><guid>/blog/2022/04/07/ssi-empowered-identity-provider/</guid><description>
&lt;p>Utilizing SSI wallets and verifiable credentials in OIDC authentication flows has been an
interesting research topic for &lt;a href="https://findy-network.github.io">our team&lt;/a> already for a while now.
As said, &lt;a href="https://openid.net/connect/">the OIDC protocol&lt;/a> is popular. Countless web services sign
their users in using OIDC identity providers. And indeed, it provides many benefits, as it
simplifies the authentication for service developers and end-users. The developers do not have
to reinvent the authentication wheel and worry about storing username/password information.
The users do not have to maintain countless digital identities with several different passwords.&lt;/p>
&lt;figure>
&lt;img src="https://pbs.twimg.com/media/FA_yO0jUcAM2fMa.jpg" width="500px"/> &lt;figcaption>
&lt;p>
&lt;a href="https://indieweb.org/NASCAR_problem">&lt;small>Example of login page with support for multiple identity providers.&lt;br/> Image source: IndieWeb NASCAR Problem&lt;/small>&lt;/a>&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>However, &lt;a href="https://medium.com/mattr-global/if-you-build-an-island-youll-need-a-boat-537f48525edc">the protocol is not flawless&lt;/a>,
and it seems evident that using verified data would fix many of the known weaknesses.&lt;/p>
&lt;p>&lt;strong>Our most significant concerns for current OIDC protocol practices are related to privacy.&lt;/strong>&lt;/p>
&lt;p>Let&amp;rsquo;s suppose that our imaginary friend Alice uses an application, say a dating service,
that provides a Facebook login button for its users. Each time Alice logs in, Facebook becomes
aware that Alice uses the dating service. Depending on the service authentication model, i.e., how
often the service requires users to reauthenticate, it can also
learn a great deal at which time and how often Alice is using the service.&lt;/p>
&lt;p>Alice probably didn&amp;rsquo;t want to share this data with Facebook and did this
unintentionally. Even worse, Alice probably uses a similar login approach with other applications.
Little by little, Facebook learns about which applications Alice is using
and how often. Moreover, as applications usually provide a limited amount of login options,
most users choose the biggest identity providers such as Facebook and Google.
The big players end up collecting an enormous amount of data over users.&lt;/p>
&lt;h2 id="how-would-ssi-and-verified-data-change-the-scenario">How Would SSI and Verified Data Change the Scenario?&lt;/h2>
&lt;p>In the traditional OIDC flow, identity providers hold the sensitive end-user data and personally
identifiable information. Yet, this is not the case with the SSI model, where the user owns
her data and stores it in her digital wallet as verifiable credentials. In the SSI-enabled
authentication process, instead of typing username and password to the identity provider login
form, &lt;em>the user presents verifiable proof of the needed data&lt;/em>. No third parties are necessary for
the login to take place.&lt;/p>
&lt;p>Furthermore, the transparent proof presentation process lets the user know which data fields
the application sees. In the traditional flow, even though the service usually asks if the user
wishes to share her profile information, the data is transferred server-to-server invisibly.
The level of transparency depends on the identity provider’s goodwill and service design quality.
In the proof presentation, the wallet user sees in detail which attributes she shares with the application.&lt;/p>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 635px">
&lt;img class="card-img-top" src="/blog/2022/04/07/ssi-empowered-identity-provider/attributes_hu577c77636a56cb15542063a613041bde_197358_625x625_fit_catmullrom_3.png" width="625" height="436">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>In the proof presentation, the wallet user sees in detail which attributes she shares with the application.&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>The verifiable credentials technology would even allow computations on the user data without
revealing it. For example, if we assume that Alice has a credential about her birthdate
in her wallet, she could prove that she is over 18 without exposing her birthdate when registering
to the dating service.&lt;/p>
&lt;h2 id="midway-solution-for-speeding-up-adoption">Midway Solution for Speeding up Adoption&lt;/h2>
&lt;p>The ideal SSI-enabled OIDC login process wouldn&amp;rsquo;t have any identity provider role, or actually,
the user would be the identity provider herself. The current identity provider (or any other service
holding the needed identity data) would issue the credential to the user’s wallet before any logins.
After the issuance, the user could use the data directly with the client applications
as she wishes without the original issuer knowing it.&lt;/p>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 935px">
&lt;img class="card-img-top" src="/blog/2022/04/07/ssi-empowered-identity-provider/shift.drawio_hu78bea64891b44cd1059188eeedccda7e_263949_925x925_fit_catmullrom_3.png" width="925" height="456">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>In SSI-Enabled OIDC login flow there is no need for traditional identity provider
with user data silos.&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>The OIDC extension
&lt;a href="https://openid.net/specs/openid-connect-self-issued-v2-1_0.html">SIOP (Self-Issued OpenID Provider)&lt;/a>
tries to reach this ambitious goal. The specification defines how the client applications can verify
users' credential data through the renewed OIDC protocol. Unfortunately, implementing SIOP would require
considerable changes to existing OIDC client applications.&lt;/p>
&lt;p>As adapting these changes to OIDC client applications is undoubtedly slow, we think a midway
solution not requiring too many changes to the OIDC clients would be ideal for speeding up
the SSI adoption. The identity provider would work as an SSI proxy in this solution, utilizing
SSI agent capabilities. Instead of storing the sensitive user data in its database, the provider
would verify the user&amp;rsquo;s credential data and deliver it to the client applications using the same
API interfaces as traditional OIDC.&lt;/p>
&lt;h2 id="findy-agency-under-test">Findy Agency under Test&lt;/h2>
&lt;p>In the summer of 2020, our team did some initial proofs-of-concept around this subject.
The experiments were successful, but our technology stack
&lt;a href="https://findy-network.github.io/blog/2021/08/11/announcing-findy-agency/">has matured&lt;/a> since then.
We decided to rewrite the experiments on top of our latest stack and take a closer look at this topic.&lt;/p>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 835px">
&lt;img class="card-img-top" src="/blog/2022/04/07/ssi-empowered-identity-provider/overview.drawio_hu9ec8414f57daa6762ef8cf1942977f73_180118_825x825_fit_catmullrom_3.png" width="825" height="427">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>Overview of the midway solution process participants&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>Other teams have created &lt;a href="https://github.com/bcgov/vc-authn-oidc/blob/main/docs/README.md">similar demos&lt;/a>
in the past but using different SSI technology stacks. Our target was to test our
&lt;a href="https://github.com/findy-network/findy-agent-api">Findy Agency gRPC API&lt;/a> hands-on. Also, our
web wallet&amp;rsquo;s user experience is somewhat different from other SSI wallets. The web wallet
can be used securely with the browser without installing mobile applications. Furthermore,
the core concept of our wallet app is the chat feature, which is almost missing altogether from
other SSI wallet applications. We think that the chat feature has an essential role in creating
an excellent user experience for SSI wallet users.&lt;/p>
&lt;h2 id="demo">Demo&lt;/h2>
&lt;p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/V5FWX0g3HVk" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;em>The demo video shows how the technical PoC works on localhost setup.
The user logs in to a protected service using the web wallet.&lt;/em>&lt;/p>
&lt;p>The basic setup for the demo is familiar to OIDC utilizers. The end-user uses a browser
on the laptop and wishes to log in to a protected web service. The protected sample service
for this demo playing the OIDC client role is called
the &lt;a href="https://github.com/findy-network/findy-issuer-tool">&amp;ldquo;issuer tool&amp;rdquo;&lt;/a>. The service has configured
&lt;a href="https://github.com/findy-network/findy-oidc-provider">an SSI-enabled identity provider&lt;/a>
as a login method. It displays the button “Login via credential” on its login page.
The service redirects the user to the identity provider login page with a button click.&lt;/p>
&lt;p>Then the flow changes from the usual OIDC routine. Before the login, the user has already acquired
the needed data (an FTN - Finnish Trust Network credential) in her SSI wallet. She uses her
&lt;a href="https://github.com/findy-network/findy-wallet-pwa">web wallet&lt;/a> on her mobile device to read
the connection invitation as a QR code from the login page
to begin the DIDComm communication with the identity provider. The identity provider will then
verify the user’s credential and acquire the data the client application needs for the login.
The rest of the flow continues as with traditional OIDC, and finally, the client application
redirects the user to the protected service. The entire process sequence is below in detail:&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="/blog/2022/04/07/ssi-empowered-identity-provider/sequence.svg"/>
&lt;/figure>
&lt;em>Step-by-step sequence for the login process&lt;/em>&lt;/p>
&lt;h2 id="implementation">Implementation&lt;/h2>
&lt;p>The demo services utilize OIDC JS helper libraries
(&lt;a href="https://github.com/panva/node-openid-client">client&lt;/a>,
&lt;a href="https://github.com/panva/node-oidc-provider">server&lt;/a>). We implemented the client application
integration similarly to any OIDC login integration, so there was no need to add any dedicated code
for SSI functionality. For the identity provider, we took
&lt;a href="https://github.com/panva/node-oidc-provider/tree/main/example">the JS OIDC provider sample code&lt;/a>
as the basis and extended the logic with
&lt;a href="https://github.com/findy-network/findy-oidc-provider/blob/master/src/agent/index.js">the SSI-agent controller&lt;/a>.
The number of needed code changes was relatively small, which showed us that these integrations to
the “legacy” world are possible and &lt;em>easy&lt;/em> to implement with an SSI agency that provides
a straightforward API.&lt;/p>
&lt;p>All of the codes are available on GitHub
(&lt;a href="https://github.com/findy-network/findy-issuer-tool">client&lt;/a>,
&lt;a href="https://github.com/findy-network/findy-oidc-provider">provider&lt;/a>)
so that you can take a closer look or even set up the demo on your local computer.&lt;/p>
&lt;p>&lt;em>We will continue our exploration journey with the verified data and the OIDC world, so stay tuned!&lt;/em>&lt;/p></description></item><item><title>Blog: Replacing Indy SDK</title><link>/blog/2022/03/14/replacing-indy-sdk/</link><pubDate>Mon, 14 Mar 2022 00:00:00 +0000</pubDate><guid>/blog/2022/03/14/replacing-indy-sdk/</guid><description>
&lt;p>&lt;a href="https://findy-network.github.io/blog/2021/09/08/travelogue/">Once again, we are at the technology
crossroad&lt;/a>: we have
to decide how to proceed with our SSI/DID research and development. Naturally,
the business potential is the most critical aspect, but the research subject has
faced the phase where &lt;em>we have to change the foundation&lt;/em>.&lt;/p>
&lt;p>&lt;img src="https://findy-network.github.io/blog/2021/09/08/travelogue/TechTree_hucb3e947ff7ac3c644fea522c44e98b42_1584631_1991x0_resize_catmullrom_3.png" alt="SSI Layers">&lt;/p>
&lt;p align = "center"> Our Technology Tree - &lt;a
href="https://findy-network.github.io/blog/2021/09/08/travelogue/">
Travelogue&lt;/a>&lt;/p>
&lt;p>Changing any foundation could be an enormous task, especially when a broad
spectrum of technologies is put together. (Please see the picture above).
Fortunately, we have taken care of this type of a need early in the design when
the underlying foundation, &lt;a href="https://github.com/hyperledger/indy-sdk">Indy SDK&lt;/a>
is double wrapped:&lt;/p>
&lt;ol>
&lt;li>We needed a Go wrapper for &lt;code>libindy&lt;/code> itself, i.e. &lt;em>language wrapping&lt;/em>.&lt;/li>
&lt;li>At the beginning of the &lt;code>findy-agent&lt;/code> project, we tried to find agent-level
concepts and interfaces for multi-tenant agency use, i.e. &lt;em>conceptual
wrapping&lt;/em>.&lt;/li>
&lt;/ol>
&lt;p>This post is peculiar because I&amp;rsquo;m writing it up front and not just reporting
something that we have been studied and verified carefully.&lt;/p>
&lt;blockquote>
&lt;p>&amp;ldquo;I am in a bit of a paradox, for I have assumed that there is no good in
assuming.&amp;rdquo; - Mr. Eugene Lewis Fordsworthe&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;m still writing this. Be patient; I&amp;rsquo;ll try to answer &lt;em>why&lt;/em> in the following chapters.&lt;/p>
&lt;h2 id="who-should-read-this">Who Should Read This?&lt;/h2>
&lt;p>You should read this if:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>You are considering jumping on the SSI/DID wagon, and you are searching good
technology platform for your SSI application. You will get selection criteria
and fundamentals from here.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You are in the middle of the development of your own platform, and you need
a concrete list of aspects you should take care of.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You are currently using Indy SDK, and you are designing your architecture based
on &lt;a href="https://github.com/hyperledger/aries">Aries reference architecture&lt;/a>
and its shared libraries.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You are interested to see the direction the &lt;code>findy-agent&lt;/code> DID agency core is
taking.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="indy-sdk-is-obsolete">Indy SDK Is Obsolete&lt;/h2>
&lt;p>Indy SDK and related technologies are obsolete, and they are proprietary already.&lt;/p>
&lt;p>We have just reported that our Indy SDK based DID agency is &lt;a href="https://github.com/hyperledger/aries-rfcs/blob/main/concepts/0302-aries-interop-profile/README.md">AIP
1.0&lt;/a>
compatible, and everything is wonderful. How in the hell did Indy SDK become
obsolete and proprietary in a month or so?&lt;/p>
&lt;p>Well, let&amp;rsquo;s start from the beginning. I did write the following on January
19th 2022:&lt;/p>
&lt;p>&lt;strong>Indy SDK is on the sidetrack from the DID state of the art.&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Core concepts as explicit entities are missing: &lt;em>DID method&lt;/em>,
&lt;em>DID resolving&lt;/em>, &lt;em>DID Documents&lt;/em>, etc.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Because of the previous reasons, the &lt;em>API&lt;/em> of Indy SDK is not optimal
anymore.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>libindy&lt;/code> is too much framework than a library, i.e. it assumes how things
will be tight together, it tries to do too much in one function, or it
doesn&amp;rsquo;t isolate parts like ledger from other components like a wallet in a correct
way, etc.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Indy SDK has too many dynamic library dependencies when compared to
what those libraries achieve.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="the-problem-statement-summary">The Problem Statement Summary&lt;/h2>
&lt;p>We have faced two different but related problems:&lt;/p>
&lt;ol>
&lt;li>Indy SDK doesn&amp;rsquo;t align with the current W3C and Aries specifications.&lt;/li>
&lt;li>&lt;a href="https://findy-network.github.io/blog/2022/03/05/the-missing-network-layer-model/">The W3C and Aries specifications are too broad and lack clear
focus&lt;/a>.&lt;/li>
&lt;/ol>
&lt;h3 id="did-specifications">DID Specifications&lt;/h3>
&lt;p>I cannot guide the work of W3C or Aries, but I can participate in our own team&amp;rsquo;s
decision making, and we will continue on the road where we&amp;rsquo;ll concentrate our
efforts to &lt;a href="https://identity.foundation/didcomm-messaging/spec/">DIDComm&lt;/a>,
which will mean that we&amp;rsquo;ll keep the same Aries protocols implemented as we have
now, but with the latest DID message formats:&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://github.com/hyperledger/aries-rfcs/blob/main/features/0023-did-exchange/README.md">DID
Exchange&lt;/a>
to build a DIDComm connection over an invitation or towards public DID.&lt;/li>
&lt;li>&lt;a href="https://github.com/hyperledger/aries-rfcs/blob/main/features/0453-issue-credential-v2/README.md">Issue
Credential&lt;/a>
to use a DIDComm connection to issue credentials to a holder.&lt;/li>
&lt;li>&lt;a href="https://github.com/hyperledger/aries-rfcs/blob/main/features/0454-present-proof-v2/README.md">Present
Proofs&lt;/a>
to present proof over a DIDComm connection.&lt;/li>
&lt;li>&lt;a href="https://github.com/hyperledger/aries-rfcs/blob/main/features/0095-basic-message/README.md">Basic
Message&lt;/a>
to have private conversations over a DIDComm connection.&lt;/li>
&lt;li>&lt;a href="https://github.com/hyperledger/aries-rfcs/blob/main/features/0048-trust-ping/README.md">Trust
Ping&lt;/a>
to test a DIDComm connection.&lt;/li>
&lt;/ol>
&lt;p>Keeping the same protocol set might sound simple, but unfortunately, it&amp;rsquo;s not because
Indy SDK doesn&amp;rsquo;t have, e.g. a concept for &lt;code>DID Method&lt;/code>. At the end of the January
2022, no one has implemented the &lt;code>did:indy&lt;/code> method either, and its specification is
still in work-in-progress.&lt;/p>
&lt;p>The methods we&amp;rsquo;ll support first are &lt;code>did:peer&lt;/code> and &lt;code>did:key&lt;/code>. The first is
evident because our current Indy implementation builds almost identical pairwise
connections with Indy DIDs. The &lt;code>did:key&lt;/code> method replaces all public keys in
DIDComm messages. It has other use as well.&lt;/p>
&lt;p>The &lt;code>did:web&lt;/code> method is probably the next. It gives us an
implementation baseline for the actual &lt;em>super&lt;/em> &lt;a href="https://blockchaincommons.github.io/did-method-onion/">DID Method
&lt;code>did:onion&lt;/code>&lt;/a>.
In summary, onion routing gives us &lt;a href="https://findy-network.github.io/blog/2022/03/05/the-missing-network-layer-model/">a new transport layer (OSI
L4)&lt;/a>.&lt;/p>
&lt;p>We all know how difficult adding security and privacy to the internet&amp;rsquo;s (TCP/IP)
network layers are
(&lt;a href="https://www.securityweek.com/top-five-worst-dns-security-incidents">DNS&lt;/a>,
&lt;a href="https://neeva.com/learn/data-privacy-4-common-issues-and-how-to-solve-them">etc.&lt;/a>).
But replacing the transport layer with a new one is the best solution. Using
&lt;strong>onion addresses for &lt;a href="https://ldapwiki.com/wiki/DID%20Service%20Endpoint">the DID Service
Endpoints&lt;/a> solves
routing in own decoupled layer&lt;/strong> which reduces complexity tremendously.&lt;/p>
&lt;p>&lt;img src="https://i.imgflip.com/xk14q.jpg" alt="air">&lt;/p>
&lt;blockquote>
&lt;p>&amp;ldquo;In some sense IP addresses are not even meaningful to Onion Services: they
are not even used in the protocol.&amp;rdquo; - &lt;a href="https://community.torproject.org/onion-services/overview/">Onion
Services&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;h3 id="indy-sdk-replacement">Indy SDK Replacement&lt;/h3>
&lt;p>Indy SDK is obsolete and proprietary. It&amp;rsquo;s not based on &lt;a href="https://www.w3.org/TR/did-core/">the current W3C DID core
concepts&lt;/a>. That makes it too hard to build reasonable
solutions over Indy SDK without reinventing the wheel. We have decided to
architect the ideal solution first and then make the selection criteria from it.
With the requirements, we start to select candidates for our crypto libraries.&lt;/p>
&lt;p>We don&amp;rsquo;t want to replace Indy SDK right away. We want to keep it until we don&amp;rsquo;t
need it anymore. When all parties have changed their verified credential formats
according to the standard, we decide again if we can drop it.&lt;/p>
&lt;h2 id="putting-all-together">Putting All Together&lt;/h2>
&lt;p>We described our problems in &lt;a href="#the-problem-statement-summary">the problem statement summary&lt;/a>.
We will put things together in the following chapters and present our problem-solving strategy.&lt;/p>
&lt;p>First, we need to align our current software solution to W3C specifications.
Aries protocols are already covered. Secondly, we need to find our way for
specification issues like selecting &lt;a href="https://findy-network.github.io/blog/2022/03/05/the-missing-network-layer-model/">proper DID methods to
support&lt;/a>.&lt;/p>
&lt;p>The missing (from Indy SDK) DID core concepts: &lt;code>DID&lt;/code>, &lt;code>DID document&lt;/code>, &lt;code>DID method&lt;/code>, &lt;code>DID resolving&lt;/code>, will be the base for &lt;strong>our target architecture&lt;/strong>. The following UML
diagram presents our high-level conceptual model of these concepts and
their relations.&lt;/p>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 1210px">
&lt;img class="card-img-top" src="/blog/2022/03/14/replacing-indy-sdk/cover-classes_hu68169d6d854aa04b756d328f13ce2f4e_177274_1200x0_resize_catmullrom_3.png" width="1200" height="1046">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>Agency DID Core Concepts&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>The class diagram shows that &lt;code>DIDMethodBase&lt;/code> is a critical abstraction because
it hides implementation details together with the interfaces it extends. Our
current agent implementation uses &lt;em>factory pattern&lt;/em> with &lt;em>new-by-name&lt;/em>, which
allows our system to read protocol streams and implicitly create native Go objects.
That has proven to be extremely fast and programmer-friendly. We will use
a similar strategy in our upcoming &lt;code>DID method&lt;/code> and &lt;code>DID resolving&lt;/code> solutions.&lt;/p>
&lt;h3 id="resolving">Resolving&lt;/h3>
&lt;p>The following diagram is our first draft of how we will integrate DID document
resolving to our agency.&lt;/p>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 810px">
&lt;img class="card-img-top" src="/blog/2022/03/14/replacing-indy-sdk/resolve_hu68843e186b7e94ed4412e82ad891eddc_57470_800x0_resize_catmullrom_3.png" width="800" height="988">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>Agency DID Core Concepts&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>The sequence diagram is a draft where &lt;code>did:key&lt;/code> is solved. The method is solved
by computation. It doesn&amp;rsquo;t need persistent storage for &lt;code>DID documents&lt;/code>. However,
the drawing still illustrates our idea to have one internal resolver (&lt;em>factory&lt;/em>)
for everything. That gives many advantages like caching, but it also
keeps things simple and testable.&lt;/p>
&lt;h3 id="building-pairwise----did-exchange">Building Pairwise &amp;ndash; DID Exchange&lt;/h3>
&lt;p>You can have an explicit invitation
(&lt;a href="https://github.com/hyperledger/aries-rfcs/blob/main/features/0434-outofband/README.md">OOB&lt;/a>)
protocol or you can just have &lt;a href="https://findy-network.github.io/blog/2022/03/05/the-missing-network-layer-model/#what-is-a-public-did">a public
DID&lt;/a>
that implies an invitation just by existing and resolvable the way that leads
service endpoints. Our resolver handles DIDs and DID documents and &lt;em>invitations&lt;/em>
as well. It&amp;rsquo;s essential because our existing applications have proven that
a pairwise connection is the fundamental building block of the DID system.&lt;/p>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 810px">
&lt;img class="card-img-top" src="/blog/2022/03/14/replacing-indy-sdk/pairwise_hu8e4606477e1f49816dced191085ba872_138973_800x0_resize_catmullrom_3.png" width="800" height="826">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>Agency DID Core Concepts&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>We should be critical just to avoid complexity. If the goal is to reuse existing
pairwise (connection), and the most common case is a public website, should we
leave that for public DIDs and try not to solve by invitation? When public DIDs
would scale and wouldn&amp;rsquo;t be correlatable, we might be able to simplify
invitations at least? Or, should we think if we really need &lt;em>connectable&lt;/em> public
DIDs? Or maybe we don&amp;rsquo;t need both of them anymore, just another?&lt;/p>
&lt;p>&lt;strong>Our target architecture&lt;/strong> helps us to find answers to these questions. It also
allows us to keep track of non-functional requirements like &lt;em>modifiability,
scalability, security, privacy, performance, simplicity, testability&lt;/em>. These are
the most important ones, and everyone is equally important to us.&lt;/p>
&lt;h2 id="existing-sdk-options">Existing SDK Options?&lt;/h2>
&lt;p>Naturally, Indy SDK is not the only solution for SSI/DID. When the Aries project
and its goals were published, most of us thought that replacing SDKs for Indy
would come faster. Unfortunately, that didn&amp;rsquo;t happen, and there are many reasons
for that.&lt;/p>
&lt;p>Building software has many internal &amp;lsquo;ecosystems&amp;rsquo; mainly directed by programming
languages. For instance, it&amp;rsquo;s unfortunate that gophers behave like managed
language programmers and rarely use pure native binary libraries because we lose
too many good Go features. For example, we would have compromised in super-fast
builds, standalone binaries, broad platform support, etc. They might sound like
small things, but they aren&amp;rsquo;t. For example, the container image sizes for
standalone Go binaries are almost the same as the original Go binary.&lt;/p>
&lt;p>It is easier to keep your Go project only written in Go. Just one
&lt;a href="https://en.wikipedia.org/wiki/Application_binary_interface">ABI&lt;/a> library usage
would force you to follow the binary dependency tree, and you could not use
standalone Go binary. If you can find a module just written in Go, you select
that even it would be some sort of a compromise.&lt;/p>
&lt;p>That&amp;rsquo;s been one reason we have to build our own API with gRPC. That will offer
the best from both worlds and allow efficient polyglot usage. I hope others do
the same and use modern API technologies with local/remote transparency.&lt;/p>
&lt;h2 id="we-are-going-to-evaluate-afgo">We Are Going To Evaluate AFGO&lt;/h2>
&lt;p>Currently, the Aries Framework Go seems to be the best &lt;em>evaluation&lt;/em> option for us
because:&lt;/p>
&lt;ul>
&lt;li>It&amp;rsquo;s written in Go, and all its dependencies are native Go packages.&lt;/li>
&lt;li>It follows the Aries specifications by the book.&lt;/li>
&lt;/ul>
&lt;p>Unfortunately, it also has the following problems:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>It&amp;rsquo;s is &lt;em>a framework&lt;/em> which typically means all or nothing, i.e. you have to
use the whole framework because it takes care of everything, and it only offers
you extension points where you can put your application handlers. A
framework is much more complex to replace than a library.&lt;/p>
&lt;p>&lt;img src="https://csharpcorner-mindcrackerinc.netdna-ssl.com/UploadFile/a85b23/framework-vs-library/Images/DqCkT.png" alt="framework vs library">&lt;/p>
&lt;p align = "center"> Difference Between Library and Framework - &lt;a
href="https://www.c-sharpcorner.com/uploadfile/a85b23/framework-vs-library/">The Blog
Post&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Its protocol state machine implementation is not as good as ours:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>It doesn&amp;rsquo;t fork protocol handlers immediately after receiving the
message payload.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>It doesn&amp;rsquo;t offer a 30.000 ft view to machines, i.e. it doesn&amp;rsquo;t seem to
be declarative enough.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>It has totally different concepts than we and Indy SDK have for the critical
entities like DID and storage like a wallet. Of course, that&amp;rsquo;s not
necessarily a bad thing. We have to check how AFGO&amp;rsquo;s concepts map to our
target architecture.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>During the first skimming of the code, a few alarms were raised primarily for
the performance.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>We will try to wrap AFGO to use it as a library and produce an interface that we
can implement with Indy SDK and AFGO. This way, we can use our current core
components to implement different Aries protocols and even verifiable
credentials.&lt;/p>
&lt;p>Our agency has bought the following features and components which we have measured
to be superior to other similar DID solutions:&lt;/p>
&lt;ul>
&lt;li>Multi-tenancy model, i.e. symmetric agent model&lt;/li>
&lt;li>Fast server-side secure enclaves for KMS&lt;/li>
&lt;li>General and simple DID controller gRPC API&lt;/li>
&lt;li>Minimal dependencies&lt;/li>
&lt;li>Horizontal scalability&lt;/li>
&lt;li>Minimal requirements for hardware&lt;/li>
&lt;li>Cloud-centric design&lt;/li>
&lt;/ul>
&lt;p>We really try to avoid inventing the wheel, but with the current knowledge, we
cannot switch to AFGO. Instead, we can wrap it and use it as an independent
library.&lt;/p>
&lt;blockquote>
&lt;p>Don&amp;rsquo;t call us, we&amp;rsquo;ll call you. - &lt;a href="https://en.wiktionary.org/wiki/Hollywood_principle">Hollywood
Principle&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>We will continue to report our progress and publish the AFGO wrapper when
ready. Stay tuned, folks, something extraordinary is coming!&lt;/p></description></item><item><title>Blog: The Missing Network Layer Model</title><link>/blog/2022/03/05/the-missing-network-layer-model/</link><pubDate>Sat, 05 Mar 2022 00:00:00 +0000</pubDate><guid>/blog/2022/03/05/the-missing-network-layer-model/</guid><description>
&lt;p>&lt;a href="https://www.w3.org/TR/did-core/">The W3C&amp;rsquo;s DID Specification&lt;/a> is flawed without
&lt;strong>the network layer model&lt;/strong>.&lt;/p>
&lt;p>You might think that I have lost my mind. We have just reported that our Indy
SDK based DID agency is &lt;a href="https://github.com/hyperledger/aries-rfcs/blob/main/concepts/0302-aries-interop-profile/README.md">AIP
1.0&lt;/a>
compatible, and everything is wonderful. What&amp;rsquo;s going on?&lt;/p>
&lt;p>Well, let&amp;rsquo;s start from the beginning. I did write the following list on January
19th 2022:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Indy SDK doesn&amp;rsquo;t align the current W3C and Aries specifications.&lt;/p>
&lt;ul>
&lt;li>Core concepts (below) as explicit entities are missing: &lt;em>DID method&lt;/em>,
&lt;em>DID resolving&lt;/em>, &lt;em>DID Documents&lt;/em>, etc.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://www.w3.org/TR/did-core/diagrams/did_brief_architecture_overview.svg" alt="DID Concepts">&lt;/p>
&lt;p align = "center"> DID Concepts - &lt;a href="https://www.w3.org/TR/did-core/diagrams/did_brief_architecture_overview.svg">www.w3.org&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>No one in the SSI industry seems to be able to find perfect focus.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>There are too few solutions running in production (yes, globally as well)
which would give us needed references to drive a good design from.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://lists.w3.org/Archives/Public/public-new-work/2021Sep/0000.html">Standardization in W3C didn&amp;rsquo;t proceed as it
should&lt;/a>.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Findings during our study of SSI/DID and others in the industry.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>We don&amp;rsquo;t need a ledger to solve
&lt;a href="https://github.com/SmithSamuelM/Papers/blob/master/whitepapers/10-ssi-key-management.pdf">self-certified&lt;/a>
identities.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>We don&amp;rsquo;t need human &lt;em>memorable&lt;/em> identifiers. (memorable \(\ne\) meaningful
\(\ne\) typeable \(\ne\) codeable)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>We &lt;em>rarely&lt;/em> need an identifier just for &lt;em>referring&lt;/em> but we continuously need
&lt;em>self-certified identifiers for secure communication&lt;/em>: should we first fully
solve the communication problem and not the other way around?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Trust always seems to lead back to a specific type of centralization. There are
many existing studies like web-of-trust that should at least take in a
review. &lt;a href="https://www.weboftrust.info/">Rebooting Web-of-Trust&lt;/a> is an excellent
example of that kind of work.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>We must align the SSI/DID technology to the current state of the art like
Open ID and federated identity providers. &lt;a href="https://openid.net/specs/openid-connect-self-issued-v2-1_0.html">Self-Issued OpenID Provider
v2&lt;/a>
the protocol takes steps in the right direction and will work as a bridge.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h2 id="w3c-did-specification-problems">W3C DID Specification Problems&lt;/h2>
&lt;p>Now, February 20th 2022, the list is still valid, but now, when we have dug
deeper, we have learned that the DID W3C &amp;ldquo;standard&amp;rdquo; has its flaws itself.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>It&amp;rsquo;s far too complex and redundant &amp;ndash; the scope is too broad.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>There should not be so many DID methods. &lt;a href="https://lists.w3.org/Archives/Public/public-new-work/2021Sep/0000.html">&amp;ldquo;No practical interoperability.&amp;rdquo; &amp;amp;
&amp;ldquo;Encourages divergence rather than
convergence.&amp;quot;&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>For some reason, &lt;a href="https://www.w3.org/TR/did-core/">DID-core&lt;/a> doesn&amp;rsquo;t
cover protocol specifications, but protocols are in &lt;a href="https://github.com/hyperledger/aries-rfcs/blob/main/index.md">Aries
RFCs&lt;/a>. You&amp;rsquo;ll
face the problem in &lt;a href="https://identity.foundation/peer-did-method-spec/">the DID peer method
specification&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>It misses layer structures typical for network protocols. When you start to
implement it, you notice that there are no network layers to help you to hide
abstractions. Could we have &lt;a href="https://www.networkworld.com/article/3239677/the-osi-model-explained-and-how-to-easily-remember-its-7-layers.html">OSI
layer&lt;/a>
mappings or similar at least? (Please see the chapter &lt;a href="#the-missing-layer---fixing-the-internet">The Missing Layer - Fixing The
Internet&lt;/a>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Many performance red flags pop up when you start to engineer the
implementation. Just think about &lt;a href="https://brooker.co.za/blog/2021/04/19/latency.html">tail
latency&lt;/a> in DID resolving
and you see it. Especially if you think &lt;a href="https://lig-membres.imag.fr/loiseapa/pdfs/2015/Hours-etal_ImpactDNSCausal_ITC2015.pdf">the performance demand of the
DNS&lt;/a>.
The comparison is pretty fair.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="no-need-for-the-ledger">No Need For The Ledger&lt;/h2>
&lt;p>The &lt;code>did:onion&lt;/code> method is currently the only straightforward way to build
self-certified &lt;em>public&lt;/em> DIDs that cannot be correlated. The &lt;code>did:web&lt;/code> is
analogue, but it doesn&amp;rsquo;t offer privacy as itself. However, it provides privacy
for the individual agents through &lt;a href="https://github.com/w3c/did-core/issues/539">&lt;em>herd privacy&lt;/em> if DID specification doesn&amp;rsquo;t
fail in it&lt;/a>.&lt;/p>
&lt;p>Indy stores &lt;a href="https://docs.cheqd.io/node/architecture/adr-list/adr-008-identity-resources">credential definitions and
schemas&lt;/a>
to the ledger addition to public DIDs. Nonetheless, when &lt;a href="https://www.evernym.com/blog/bbs-verifiable-credentials/">verifiable credentials
move to use BBS+ credential definitions aren&amp;rsquo;t
needed&lt;/a> and schemas
can be read, e.g. from &lt;a href="schema.org">schema.org&lt;/a>. Only those DID methods need
a ledger that is using a ledger as &lt;em>public DID&amp;rsquo;s&lt;/em> &lt;a href="https://findy-network.github.io/blog/2021/11/09/anchoring-chains-of-trust/">trust
anchor&lt;/a>
and source of truth.&lt;/p>
&lt;h3 id="what-is-a-public-did">What Is A Public DID?&lt;/h3>
&lt;p>It&amp;rsquo;s a DID who&amp;rsquo;s DIDDoc you can solve without an
&lt;a href="https://github.com/hyperledger/aries-rfcs/blob/b3a3942ef052039e73cd23d847f42947f8287da2/features/0434-outofband/README.md">invitation&lt;/a>.&lt;/p>
&lt;p>The &lt;code>did:key&lt;/code> is superior because it is complete. You can compute (solve) a DID
document by receiving a valid DID key identifier. No third party or additional
source of truth is needed. However, we cannot communicate with the &lt;code>did:key&lt;/code>&amp;rsquo;s
holder because the DIDDoc doesn&amp;rsquo;t include &lt;a href="https://www.w3.org/TR/did-core/#services">service
endpoints&lt;/a>. So, there is no one
listening and nothing to connect to.&lt;/p>
&lt;p>Both &lt;code>did:onion&lt;/code>&amp;rsquo;s and &lt;code>did:web&lt;/code>&amp;rsquo;s DIDDocs can include service endpoints because
they can offer the DID document by their selves from their own servers. We must
remember that the DID document offers &lt;a href="https://www.w3.org/TR/did-core/#verification-methods">verification
methods&lt;/a> which can be used
to build the actual &lt;a href="https://findy-network.github.io/blog/2021/11/09/anchoring-chains-of-trust/">cryptographic
trust&lt;/a>.&lt;/p>
&lt;h2 id="how-to-design-best-agency">How To Design Best Agency?&lt;/h2>
&lt;p>How to do it right now when &lt;em>we don&amp;rsquo;t have a working standard or de facto
specifications&lt;/em>? We have thought that for a long time, over three years for now.&lt;/p>
&lt;p>I have thought and compared SSI/DID networks and solutions. I think we need to
have a layer model similar to the OSI network model to handle complexity.
Unfortunately, the famous trust-over-IP picture below isn&amp;rsquo;t the one that is
missing:&lt;/p>
&lt;p>&lt;img src="https://blockchain.tno.nl/media/18029/figure_1_the_toip_technology_stack_and_its_four_layers_20042021_1200_675.png?anchor=center&amp;amp;mode=crop&amp;amp;quality=90&amp;amp;width=1200&amp;amp;slimmage=true&amp;amp;rnd=132633967270000000" alt="SSI Layers">&lt;/p>
&lt;p align = "center"> The ToIP technology stack and its four layers - &lt;a
href="https://blockchain.tno.nl/blog/self-sovereign-communication/">SSI
Communication&lt;/a>&lt;/p>
&lt;p>Even though the ToIP has a layer model, it doesn&amp;rsquo;t help us build technical
solutions. It&amp;rsquo;s even more unfortunate that &lt;em>many in the industry think that
it&amp;rsquo;s the network layer model when it&amp;rsquo;s not&lt;/em>. It&amp;rsquo;s been making communication between
different stakeholders difficult because we see things differently, and we don&amp;rsquo;t
share common ground detailed enough.&lt;/p>
&lt;p>Luckily, I found &lt;a href="https://medium.com/decentralized-identity/the-self-sovereign-identity-stack-8a2cc95f2d45">a blog
post&lt;/a>
which seems to be the right one, but I didn&amp;rsquo;t find any follow-up work.
Nonetheless, we can use it as a reference and proof that there exists this kind of
need.&lt;/p>
&lt;p>The following picture is from the blog post. As we can see, &lt;em>it includes
problems&lt;/em>, and the weirdest one is the &lt;em>missing OSI mapping&lt;/em>. Even the post
explains how vital the layer model is for &lt;em>interoperability&lt;/em> and &lt;em>portability&lt;/em>.
Another maybe even more weird mistake is mentioning that layers could &lt;em>be
coupled&lt;/em> when the whole point of layered models is to have &lt;strong>decoupled layers&lt;/strong>.
Especially when building &lt;strong>privacy holding technology&lt;/strong>, it should be evident
that &lt;strong>there cannot be leaks between layers&lt;/strong>.&lt;/p>
&lt;p>&lt;img src="https://miro.medium.com/max/1400/1*4zUczSBaVH-8qilvK4nKwQ.png"
width="550" />&lt;/p>
&lt;p align = "center"> The Self-sovereign Identity Stack - &lt;a
href="https://medium.com/decentralized-identity/the-self-sovereign-identity-stack-8a2cc95f2d45">The Blog
Post&lt;/a>&lt;/p>
&lt;h3 id="the-missing-layer---fixing-the-internet">The Missing Layer - Fixing The Internet&lt;/h3>
&lt;p>The following picture illustrates mappings from the OSI model through protocols to TCP/IP
model.&lt;/p>
&lt;p>&lt;img src="https://www.dummies.com/wp-content/uploads/296299.image0.jpg" alt="TCP/IP and OSI">&lt;/p>
&lt;p>We all know that the internet was created without security and privacy, and still,
it&amp;rsquo;s incredibly successful and scalable. From the layer model, it&amp;rsquo;s easy to see
that security and privacy solutions should be put under the transport layer to
allow all of the current applications to work without changes. But it&amp;rsquo;s not
enough if we want to have end-to-end encrypted and secure communication pipes.&lt;/p>
&lt;p>&lt;strong>We need to take the best of both worlds: fix as much as possible, as a low layer
as you can one layer at a time.&lt;/strong>&lt;/p>
&lt;h3 id="secure--private-transport-layer">Secure &amp;amp; Private Transport Layer&lt;/h3>
&lt;p>There is one existing solution, and others are coming:&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://www.torproject.org/">Tor&lt;/a> and its onion routing.&lt;/li>
&lt;li>&lt;a href="https://nymtech.net/">NYM&lt;/a>, etc.&lt;/li>
&lt;/ol>
&lt;p>I know that Tor has its performance problems, etc., but the point is not about
that. The point is to which network layer should handle secure and private
transport. It&amp;rsquo;s not DIDComm, and it definitely isn&amp;rsquo;t implemented as &lt;em>statical
routing like currently in DIDComm&lt;/em>. Just think about it: What it means when you
have to change your mediator or add another one, and compare it to current
TCP/IP network? It&amp;rsquo;s a no-brainer that routing should be isolated in a layer.&lt;/p>
&lt;p>The following picture shows how OSI and TCP/IP layers map. It also shows one
possibility to use onion routing instead on insecure and public TCP/IP routing
for DID communication.&lt;/p>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 1210px">
&lt;img class="card-img-top" src="/blog/2022/03/05/the-missing-network-layer-model/OSIMap_hue1e7d1ebdd8994d467756b8af57eda38_245909_1200x0_resize_catmullrom_3.png" width="1200" height="945">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>DID Communication OSI Mapping&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>The solution is secure and private, and there aren&amp;rsquo;t no leaks between layers which
could lead to correlation.&lt;/p>
&lt;h2 id="putting-all-together">Putting All Together&lt;/h2>
&lt;p>The elephant is eaten one bite at a time is a strategy we have used
successfully and continue to use here. We start with missing core
concepts: &lt;code>DID&lt;/code>, &lt;code>DID document&lt;/code>, &lt;code>DID method&lt;/code>, &lt;code>DID resolving&lt;/code>. The following
UML diagram present our high-level conceptual model of these concepts and their
relations.&lt;/p>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 1210px">
&lt;img class="card-img-top" src="/blog/2022/03/05/the-missing-network-layer-model/classes_hu68169d6d854aa04b756d328f13ce2f4e_177274_1200x0_resize_catmullrom_3.png" width="1200" height="1046">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>Agency DID Core Concepts&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>Because current DID specification allows or supports many different &lt;em>DID
Methods&lt;/em> we have to take care of them in the model. It would be naive to think
we could use only external &lt;em>DID resolver&lt;/em> and delegate DIDDoc solving.
Just for think about performance, it would be a nightmare, security issues even
more.&lt;/p>
&lt;h3 id="replacing-the-indy-sdk">Replacing the Indy SDK&lt;/h3>
&lt;p>We will publish a separate post about replacing Indy SDK or bringing other Aries
solutions as a library. What the basic strategy will be is decided
during the work. We’ll implement new concepts and implement only these DID
methods during the process:&lt;/p>
&lt;ul>
&lt;li>&lt;em>DID Key&lt;/em>, needed to replace public key references, and it&amp;rsquo;s usable for many
other purposes as well.&lt;/li>
&lt;li>&lt;em>DID Peer&lt;/em>, building pairwise connection is a foundation to DIDComm. However,
we are still figuring out the proper implementation scope for
the &lt;code>did:peer&lt;/code>.&lt;/li>
&lt;li>&lt;em>DID Web and Onion&lt;/em>, it seems that this is an excellent transition method towards
more private and sovereign &lt;code>did:onion&lt;/code> method.&lt;/li>
&lt;/ul>
&lt;p>Stay tuned. The following blog post is coming out in a week.&lt;/p></description></item><item><title>Blog: Fostering Interoperability</title><link>/blog/2022/01/19/fostering-interoperability/</link><pubDate>Wed, 19 Jan 2022 00:00:00 +0000</pubDate><guid>/blog/2022/01/19/fostering-interoperability/</guid><description>
&lt;p>Different services have different requirements and technical stacks; there are also multiple ways to implement the Aries agent support in an application. Some projects choose to rely on an Aries framework of a specific language and bundle the functionality within the service. Others might run the agent as a separate service or, as in the case of Findy Agency, as an agency that serves multiple clients.&lt;/p>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 935px">
&lt;img class="card-img-top" src="/blog/2022/01/19/fostering-interoperability/manual-test_hu2ad66fb4e4e3677b9f68960c3b1b7d49_90636_925x925_fit_catmullrom_3.png" width="925" height="486">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>Sending Aries basic messages between wallets from different technology stacks. See full demo in &lt;a href="https://www.youtube.com/watch?v=W1H7ppS2Y6M" target="_blank" rel="noopener noreferer">YouTube&lt;/a>.&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>Interoperability is a crucial element when we think about the adaptation and success of the Aries protocol. Even though the agent software might fulfill all the functional requirements and pass testing with use cases executed with a single agent technology stack, the story ending might be different when running the cases against another agent implementation. How can we then ensure that the two agents built with varying technology stacks can still work together and reach the same goals? Interoperability testing solves this problem. Its purpose is to verify that the agent software complies with the Aries protocol used to communicate between agents.&lt;/p>
&lt;h2 id="aries-interoperability-testing">Aries Interoperability Testing&lt;/h2>
&lt;p>Interoperability considerations came along quite early to the protocol work of the Aries community. The community faced similar challenges as other technical protocol developers have faced over time. When the number of Aries protocols increases and the protocol flows and messages are updated as the protocols evolve, how can the agent developers maintain compatibility with other agent implementations? The community decided to take &lt;a href="https://github.com/hyperledger/aries-rfcs/tree/main/concepts/0302-aries-interop-profile">Aries Interoperability Profiles (AIPs)&lt;/a> in use. Each AIP version defines a list of &lt;a href="https://github.com/hyperledger/aries-rfcs">Aries RFCs&lt;/a> with specific versions. Every agent implementation states which AIP version it supports and expects other implementations with the same version support to be compatible.&lt;/p>
&lt;p>To ensure compatibility, the community had &lt;a href="https://github.com/hyperledger/aries-rfcs/tree/main/concepts/0270-interop-test-suite">an idea of a test suite&lt;/a> that the agent developers could use to make sure that the agent supports the defined AIP version. The test suite would launch the agent software and run a test set that measures if the agent under test behaves as the specific protocol version requires. The test suite would generate a report of the test run, and anyone could then easily compare the interoperability results of different agents.&lt;/p>
&lt;p>At first, there were two competing test suites with different approaches to execute the tests. &lt;a href="https://github.com/hyperledger/aries-protocol-test-suite">Aries Protocol Test Suite (APTS)&lt;/a> includes an agent implementation that interacts with the tested agent through the protocol messages. On the other hand, &lt;a href="https://github.com/hyperledger/aries-agent-test-harness">Aries Agent Test Harness (AATH)&lt;/a> runs the tests operating the agent-client interface. This approach makes it possible to measure the compatibility of any two agent implementations. AATH seems to be the winning horse of the test suite race. Its test suite includes several test cases and has extensive reporting in place.&lt;/p>
&lt;h3 id="aries-agent-test-harness">Aries Agent Test Harness&lt;/h3>
&lt;p>Aries Agent Test Harness provides a BDD (behavioral driven) test execution engine and a set of tests derived from Aries RFCs. The aim is to run these tests regularly between different Aries agents (and agent frameworks) to monitor the compatibility score for each combination and catch compatibility issues.&lt;/p>
&lt;p>Harness operates the agents under test through backchannels. Backchannel is a REST interface defined by &lt;a href="https://github.com/hyperledger/aries-agent-test-harness/blob/main/docs/assets/openapi-spec.yml">an OpenAPI definition&lt;/a>, and its purpose is to pass the harness requests to the agents. The target is to handle the agent as a black box without interfering with the agent&amp;rsquo;s internal structures. Thus, the backchannel uses the agent&amp;rsquo;s client interface to pass on the harness requests.&lt;/p>
&lt;figure>
&lt;img src="https://courses.edx.org/assets/courseware/v1/571727dd6d3f57d64158c9567f0d8ff2/asset-v1:LinuxFoundationX&amp;#43;LFS173x&amp;#43;1T2020&amp;#43;type@asset&amp;#43;block/The_Aries_Agent_Test_Harness.png"/> &lt;figcaption>
&lt;p>
&lt;a href="https://learning.edx.org/course/course-v1:LinuxFoundationX&amp;#43;LFS173x&amp;#43;1T2020/home">image source: LinuxFoundationX LFS173x (CC BY 4.0)&lt;/a>&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>Harness utilizes Docker containers for testing. It launches a container based on a required agent image for each test scenario actor during the test run. Before the test run, one needs to build a single image containing all the needed agent services and the backchannel. The recipes for making each of the different agent images, i.e., Dockerfiles with the needed scripts, are stored in the AATH repository. The same repository also contains CI scripts for executing the tests regularly and generating &lt;a href="https://aries-interop.info/">an extensive test report site&lt;/a>.&lt;/p>
&lt;h2 id="interoperability-for-findy-agency">Interoperability for Findy Agency&lt;/h2>
&lt;p>One of our main themes for 2H/2021 was to verify the Aries interoperability level for Findy Agency. When I investigated the Aries interoperability tooling more, it became evident that we needed to utilize the AATH to accomplish the satisfactory test automation level.&lt;/p>
&lt;p>My first task was to create &lt;a href="https://github.com/findy-network/findy-agent-backchannel">a backchannel&lt;/a> for the harness to operate Findy Agency-hosted agents. Backchannel&amp;rsquo;s role is to convert the harness&amp;rsquo;s REST API requests to Findy Agency gRPC client interface. Another challenge was to combine the agency microservices into &lt;a href="https://github.com/findy-network/findy-agent-backchannel/blob/master/aath/Dockerfile">a single Docker image&lt;/a>. Each agency microservice runs in its dedicated container in a regular agency deployment. For AATH, I needed to bundle all of the required services into a single container, together with the backchannel.&lt;/p>
&lt;p>Once the bundle was ready, I made &lt;a href="https://github.com/hyperledger/aries-agent-test-harness/pull/341">a PR to the AATH repository&lt;/a> to include Findy Agency in the Aries interoperability test set. We decided to support AIP version 1.0, but leave out the revocation for now. Tests exposed some essential but mainly minor interoperability issues with our implementation, and we were able to solve all of the found problems quite swiftly. The tests use the latest Findy Agency release with each test run. One can monitor &lt;a href="https://aries-interop.info/findy.html">the test results for Findy Agency&lt;/a> on the test result site.&lt;/p>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 949px">
&lt;img class="card-img-top" src="/blog/2022/01/19/fostering-interoperability/results_hufa4ee9cabbf5b53022a6b7fed8ad758e_125699_939x649_fit_catmullrom_3.png" width="939" height="644">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>Test result snapshot from &lt;a href="https://aries-interop.info/" target="_blank" rel="noopener noreferer">Aries test reporting site&lt;/a>&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>In addition to interoperability testing, we currently utilize the AATH tooling for our functional acceptance testing. Whenever PR gets merged to our agency core repository that hosts the code for Aries protocol handlers, &lt;a href="https://github.com/findy-network/findy-agent/blob/master/.github/workflows/iop.yml">CI builds&lt;/a> an image of the code snapshot and runs a partial test set with AATH. The testing does not work as a replacement for unit tests but more as a last acceptance gate. The agency core runs in the actual deployment Docker container. The intention is to verify both the successful agency bootup and the functionality of the primary protocol handlers. This testing step has proven to be an excellent addition to our test repertoire.&lt;/p>
&lt;h3 id="manual-tests">Manual Tests&lt;/h3>
&lt;p>Once the interoperability test automation reached an acceptable level, my focus moved to actual use cases that I could execute between the different agents.&lt;/p>
&lt;p>My main interests were two wallet applications freely available in the app stores, &lt;a href="https://lissi.id/">Lissi Wallet&lt;/a> and &lt;a href="https://trinsic.id/trinsic-wallet/">Trinsic Wallet&lt;/a>. I was intrigued by how Findy Agency-based applications would work with these identity wallets. I also wanted to test our Findy Agency web wallet with an application from a different technology stack. &lt;a href="https://github.com/bcgov">BCGov&lt;/a> provides a freely available test network that both wallet applications support, so it was possible to execute the tests without network-related hassle.&lt;/p>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 935px">
&lt;img class="card-img-top" src="/blog/2022/01/19/fostering-interoperability/cover-demo.drawio_huf36e1a42bdf7cc78eb70fb813ee7e03b_414690_925x925_fit_catmullrom_3.png" width="925" height="537">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>Manual test setup&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>I executed the following tests:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Test 1: Findy Agency based issuer/verifier with Lissi Wallet&lt;/strong>&lt;/p>
&lt;p>A Findy Agency utilizing &lt;a href="https://github.com/findy-network/findy-issuer-tool">issuer tool&lt;/a> invites Lissi Wallet to form a pairwise connection. Issuer tool sends and verifies a credential with Lissi Wallet.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Test 2: Findy Agency Web Wallet with Trinsic Wallet&lt;/strong>&lt;/p>
&lt;p>Findy Agency Web Wallet user forms a pairwise connection with Trinsic Wallet user. Wallet applications send Aries basic messages to each other.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Test 3: ACA-Py based issuer/verifier with Findy Agency Web Wallet&lt;/strong>&lt;/p>
&lt;p>Aries Test Harness runs &lt;a href="https://github.com/hyperledger/aries-cloudagent-python">ACA-Py&lt;/a>-based agents that issue and verify credentials with Findy Agency Web Wallet.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>The practical interoperability of Findy Agency also seems to be good, as proven with these manual tests. You can find the video of the test screen recording on &lt;a href="https://www.youtube.com/watch?v=W1H7ppS2Y6M">YouTube&lt;/a>.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/W1H7ppS2Y6M" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;h2 id="next-steps">Next Steps&lt;/h2>
&lt;p>Without a doubt, Aries interoperability will be one of the drivers guiding the development of Findy Agency also in the future. With the current test harness integration, the work towards AIP2.0 is now easier to verify. Our team will continue working with the most critical Aries features relevant to our use cases. We also welcome contributions from others who see the benefit in building an OS world-class enterprise-level identity agency.&lt;/p></description></item><item><title>Blog: Anchoring Chains of Trust</title><link>/blog/2021/11/09/anchoring-chains-of-trust/</link><pubDate>Tue, 09 Nov 2021 00:00:00 +0000</pubDate><guid>/blog/2021/11/09/anchoring-chains-of-trust/</guid><description>
&lt;p>You will notice a repetitive pattern once you start to play with &lt;a href="https://en.wikipedia.org/wiki/Public-key_cryptography">public-key cryptography&lt;/a>. Everything is about chains, or more
precisely about &lt;em>the links in the chain&lt;/em>. You build these links with
public/private key pairs. Links are unidirectional, which means that if you must
link or point both ways, you need to have two key pairs, one for each
direction.&lt;/p>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 601px">
&lt;img class="card-img-top" src="/blog/2021/11/09/anchoring-chains-of-trust/RootOfChainWithAuthenticator_hud95e60c4b93d4f99683425572d5ff616_352781_591x0_resize_catmullrom_3.png" width="591" height="344">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>Crypto Chain with Authenticator&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>In this blog post, we talk mostly about protocols built with asymmetric key pairs,
but we can build immutable data structures like Merkle trees and blockchains with
&lt;a href="https://en.wikipedia.org/wiki/One-way_function">one-way functions&lt;/a> as well. We
will return to these data types in future posts by building something
interesting to replace general ledgers as DID&amp;rsquo;s
&lt;a href="https://www.w3.org/TR/did-imp-guide/#verifiable-data-registry">VDR&lt;/a>.&lt;/p>
&lt;h2 id="crypto-chain-protocols">Crypto Chain Protocols&lt;/h2>
&lt;p>We all know that the connection protocols should cover all security
issues, but protocols based on public-key cryptography might not be so
obvious &lt;em>public&lt;/em> key, you know? There are known subjects with protocols based on
asymmetric cryptography like
&lt;a href="https://en.wikipedia.org/wiki/Trust_on_first_use">trust-on-first-use&lt;/a>.&lt;/p>
&lt;img src="https://upload.wikimedia.org/wikipedia/commons/e/e7/Man_in_the_middle_attack.svg" width="400" height="10" />
&lt;p align = "center"> MITM - &lt;a href="https://upload.wikimedia.org/wikipedia/commons/e/e7/Man_in_the_middle_attack.svg">Wikipedia&lt;/a>&lt;/p>
&lt;p>It&amp;rsquo;s trivial to execute &lt;a href="https://en.wikipedia.org/wiki/Man-in-the-middle_attack">MITM&lt;/a>
attack if we cannot be sure that the public key source is the one it
should be. The industry has developed different ways to make sure that presented
details are valid. That lays down one of the most fundamental aspects of modern
cryptographic systems &amp;ndash; &lt;a href="https://en.wikipedia.org/wiki/Chain_of_trust">chain of
trust&lt;/a>.&lt;/p>
&lt;p>&lt;img src="https://upload.wikimedia.org/wikipedia/commons/0/02/Chain_Of_Trust.svg" alt="Trust Chain">&lt;/p>
&lt;p align = "center"> PKI Chain of trust - &lt;a href="https://upload.wikimedia.org/wikipedia/commons/0/02/Chain_Of_Trust.svg">Wikipedia&lt;/a>&lt;/p>
&lt;p>It is essential to understand that most of the modern security protocols use
public-key cryptography only for
&lt;a href="https://en.wikipedia.org/wiki/Authentication">authentication&lt;/a> and switch to
&lt;a href="https://en.wikipedia.org/wiki/Symmetric-key_algorithm">symmetric keys&lt;/a> during
the data transfer for performance reasons. The famous example of this kind of
protocol is &lt;a href="https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange">Diffie-Hellman&lt;/a>
where the shared secret (the symmetric key) is transported over public network.&lt;/p>
&lt;p>The &lt;a href="https://github.com/hyperledger/aries-rfcs/blob/main/concepts/0005-didcomm/README.md">DIDComm&lt;/a>
protocol is something that is not used only for authentication but
communication without sacrificing privacy. My prediction is that the current
message-oriented DIDComm protocol as a holistic transport layer is not enough.
The ongoing &lt;a href="https://github.com/decentralized-identity/didcomm-messaging">DIDComm
V2&lt;/a> mentions
potential other protocols like DIDComm Stream, DIDComm Multicast, and so forth,
but that will not be an easy task because of the current routing model, and
especially because of the privacy needs. That has been one reason we have
focused our efforts on finding a solution that would scale for all modern needs
of transporting data and keeping individuals private. For that, our cloud agency
is a perfect candidate.&lt;/p>
&lt;h2 id="symmetry-vs-asymmetry-in-protocols">Symmetry vs Asymmetry in Protocols&lt;/h2>
&lt;p>Before we go any further with DIDComm, let&amp;rsquo;s think about what it means to have
an asymmetric protocol. We know the differences between symmetric and asymmetric
cryptography. Let&amp;rsquo;s focus on communication, i.e. how we transport keys during
the protocol.&lt;/p>
&lt;p>Asymmetric protocol means that Bob can trust Alice when Alice
have given her public key to Bob, and Bob can be sure that it&amp;rsquo;s Alice whose key
he has received.&lt;/p>
&lt;p>Every time Bob needs to authenticate Alice, he asks Alice to sign
something with her private key. To make it crystal-clear, cryptographically, we
can be only sure that it&amp;rsquo;s Alice who (still) controls the private key.&lt;/p>
&lt;p>We could achieve symmetry only by that Alice has Bob&amp;rsquo;s public key as well. Now
Alice can ask Bob to sign something for the authenticity of Bob.&lt;/p>
&lt;p>Why is this important? There are several reasons for that, but the most crucial
reason is &lt;strong>the root-of-trust model&lt;/strong>. &lt;em>The last link in the crypto chain
doesn&amp;rsquo;t need to be bidirectional&lt;/em>, because &lt;em>the last private key is the
cryptographic root-of-trust, i.e. it&amp;rsquo;s passive&lt;/em>. It doesn&amp;rsquo;t need authentication
from the referrer. It&amp;rsquo;s like grounding in electronics.&lt;/p>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 601px">
&lt;img class="card-img-top" src="/blog/2021/11/09/anchoring-chains-of-trust/FirstChain_hud3a6f47a3b24ec8a1164faa8f7d21f8f_322561_591x0_resize_catmullrom_3.png" width="591" height="335">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>Crypto Chain with Grounding&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;h2 id="did-concepts">DID Concepts&lt;/h2>
&lt;p>The DID&amp;rsquo;s controller is an essential piece of the puzzle. It defines
who is the entity in the analogue world, i.e. who owns the DID cryptographically.
As long as we stay in a digital world, it is easiest to bind the controller to
its DID is by using public-key cryptography. The one who has DID controller&amp;rsquo;s
private key is the actual controller.&lt;/p>
&lt;p>For instance, an essential thing for SSI is &lt;em>a DID pairwise&lt;/em>, i.e. a secure
connection between two DIDs or &lt;a href="https://www.w3.org/TR/did-core/#dfn-service">DID
services&lt;/a>. Unfortunately, W3C&amp;rsquo;s
specifications don&amp;rsquo;t underline that enough. Probably because it concentrates on
external properties of DIDs and how the presented specification can implement
different methods. But DIDs cannot work on their own properly. They need to have
a controller, and in Aries, they have agents as well. Also, DIDs doesn&amp;rsquo;t always
present the entity they are pointing, should I say, alone. DIDs present a
&lt;em>subject&lt;/em>. A subject like an IoT device can have many different DIDs for many
different contexts.&lt;/p>
&lt;p>&lt;img src="https://www.w3.org/TR/did-core/diagrams/did_brief_architecture_overview.svg" alt="DID Concepts">&lt;/p>
&lt;p align = "center"> DID Concepts - &lt;a href="https://www.w3.org/TR/did-core/diagrams/did_brief_architecture_overview.svg">www.w3.org&lt;/a>&lt;/p>
&lt;p>In the digital world, it is expected that a controller has its controller, which
has its controller, etc. When public-key cryptography is used to verify this
controlling structure, it&amp;rsquo;s a chain with its root, the final private key, i.e.
&lt;em>the root-of-trust&lt;/em>.&lt;/p>
&lt;h2 id="didcomm-protocols">DIDComm Protocols&lt;/h2>
&lt;p>The following drawing describes a common installation scenario where an agency
based DID controller (leftmost) is implemented as verifiable automata (Finite
State Machine) and it&amp;rsquo;s controlling the DID in the agency. At the right, there
is conventional Edge Agent running in a mobile device that needs a mediator to
help the agent is accessible from the network.&lt;/p>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 1001px">
&lt;img class="card-img-top" src="/blog/2021/11/09/anchoring-chains-of-trust/BaseArchitecture_hu7901b95f6d30b4426ee2fd9a43a69eb4_499659_991x0_resize_catmullrom_3.png" width="991" height="406">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>DIDComm Installation Example&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>As we can see in the drawing below, there are many different crypto chains in
the current installation. During the study, we were most interested in the
question: what is the best way to implement the root-of-trust for the DID
managed by the multi-tenant agency. Now we have found the answer. Luckily it
existed already.&lt;/p>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 1001px">
&lt;img class="card-img-top" src="/blog/2021/11/09/anchoring-chains-of-trust/FullArchitecture_hu4b865196eaf2233c1035106d36ae8f15_921984_991x0_resize_catmullrom_3.png" width="991" height="657">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>DIDComm Installation Example and Crypto Chain&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;h2 id="fido2-authentication">FIDO2 Authentication&lt;/h2>
&lt;p>When we started to understand that the DIDComm protocol chain is not symmetric
to all directions. Or, more precisely, when we understood that there must be one
core agent for each identity domain and from that core or root agent, you should
refer to multiple &lt;strong>separated authenticators&lt;/strong>.&lt;/p>
&lt;p>Let&amp;rsquo;s see what it means to have separate authenticators. The following drawing
illustrates an oldish and problematic way of implementing, e.g. password
manager, DID controller, SSI Edge Agent, etc.&lt;/p>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 501px">
&lt;img class="card-img-top" src="/blog/2021/11/09/anchoring-chains-of-trust/PwdMgrStart_hu0f6a53f8c1ff5985fbc060f44873529a_328558_491x0_resize_catmullrom_3.png" width="491" height="549">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>Integrated Secure Enclave&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>That is how we first thought our edge agent implementation where the mobile
device&amp;rsquo;s secure element was felt as a cryptographic root-of-trust for an
identity domain that can be individual, organization, etc. However, that leads
to many unnecessary problems in protocol implementation. Most importantly, to
which part of the end-to-end protocol we should implement the use cases like:&lt;/p>
&lt;ul>
&lt;li>I want to use my identity domain from iPhone, iPad, etc. same time.&lt;/li>
&lt;li>I want to have a &amp;lsquo;forget password&amp;rsquo; -type recovery option (by doing nothing)&lt;/li>
&lt;li>I want to handle my identity domain&amp;rsquo;s keys easily. More precisely, I don&amp;rsquo;t
want to know public-key cryptography is used under the hood&lt;/li>
&lt;li>I want to have automatic backups and recovery&lt;/li>
&lt;/ul>
&lt;p>If we think about the drawing above, it&amp;rsquo;s easy to see that the presented use
cases aren&amp;rsquo;t easy to implement secure way if you have integrated a secure
element to your agent in the same machine. In case you have only one integrated
secure enclave for each edge agent, it&amp;rsquo;s near impossible.&lt;/p>
&lt;p>When we separate the secure enclave from the identity domain&amp;rsquo;s root controller
at the design level, everything seems to be set in a place as we can see in the
next drawing.&lt;/p>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 1001px">
&lt;img class="card-img-top" src="/blog/2021/11/09/anchoring-chains-of-trust/PwdMgrFull_hu4232b9d9afbfe01e455d6f477f2365d3_1060150_991x0_resize_catmullrom_3.png" width="991" height="665">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>Separated Secure Enclaves in Multiple Authenticators&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>I don&amp;rsquo;t imply that all of the other parties in the SSI/DID study scene have done
or are making the same mistake we did at the beginning. My point is that
treating secure elements as the root of the crypto chain only and not
integrating it into the software agent or the device agent is running guided us
in the right direction. That allowed us to realize that we don&amp;rsquo;t need a fully
symmetric protocol to bind the controller to the agent. All we needed was the
simplest possible thing, an authenticator, &lt;strong>a trust anchor&lt;/strong> in all potential
cases.&lt;/p>
&lt;p>That innovation brought us a possibility to use modern existing solutions and
still have an equally robust system where we have cryptographic root-of-rust.&lt;/p>
&lt;p>It&amp;rsquo;s essential to understand why we had to consider this so carefully. Should
it be just obvious? We must remember what kind of technology we were
developing. We didn&amp;rsquo;t want to make a mistake that would lead back to
centralization. For example, if we would still totally relay
&lt;a href="https://en.wikipedia.org/wiki/Public_key_infrastructure">PKI&lt;/a>, which is
centralized, we couldn&amp;rsquo;t do that.&lt;/p>
&lt;p>During the years we have studied the SSI/DID technology, we have constantly
tested the architecture with these questions:&lt;/p>
&lt;ol>
&lt;li>Could this work and be safe without any help from the current PKI? (Naturally,
it doesn&amp;rsquo;t mean that we couldn&amp;rsquo;t use individual protocols like TLS, etc. The
infrastructure is the keyword here.)&lt;/li>
&lt;li>Can a use case or a protocol action be executed peer to peer, i.e.
between only two parties? (Doesn&amp;rsquo;t still necessarily mean off-line)&lt;/li>
&lt;/ol>
&lt;h2 id="headless-fido2webauthn-authenticator">Headless FIDO2/WebAuthn Authenticator&lt;/h2>
&lt;blockquote>
&lt;p>FIDO2 is the name of the standard. WebAuthn is just browser JS API to talk to
the authenticators. So correct way to call your server is &amp;ldquo;FIDO2 Server&amp;rdquo; and
to say &amp;ldquo;Authentication with FIDO2&amp;rdquo;. -
&lt;a href="https://github.com/herrjemand/awesome-webauthn#faq">WebAuthn Resource List&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>We started our tests with the new agent API by using implementing our &lt;em>FIDO2
server&lt;/em> and by using only browsers at the beginning. When results, especially
the performance and simplicity, were so good, we decided to go further.&lt;/p>
&lt;p>The following architecture-drawing present the final deployment diagram of the
overall system. The needed FIDO2 components are marked light red, and the ones
we implemented ourselves are marked in red.&lt;/p>
&lt;p>The basic idea was to have a system-level SSO where we implemented authorization
with JWT and authentication with FIDO2 regardless of which type of the entity
needs to be authenticated: individuals, organizations, legal entities, or system
components. For us, it implicated that we needed FIDO2 for service agents, which
meant that a &lt;em>headless&lt;/em> FIDO2 Authenticator was required.&lt;/p>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 1601px">
&lt;img class="card-img-top" src="/blog/2021/11/09/anchoring-chains-of-trust/DeploymentArchitecture_hu35f0112cd83673eb5666a9c53975c3bf_2052432_1591x0_resize_catmullrom_3.png" width="1591" height="867">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>All Key Components of The System Architecture&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>Architectural requirements for the solution were quite complex because we wanted
to have security covered, not to compromise performance, and still support
polyglot development.&lt;/p>
&lt;h2 id="polyglot-authenticator-interface">Polyglot Authenticator Interface&lt;/h2>
&lt;p>FIDO2/WebAuthn specification gives a well over a description of how main
components work. Here we focus on the two most important ones. The first is the
authenticator registration flow which is presented picture below.&lt;/p>
&lt;p>&lt;img src="https://www.w3.org/TR/webauthn/images/webauthn-registration-flow-01.svg" alt="WebAuthn Registration">&lt;/p>
&lt;p align = "center"> FIDO2 Authenticator Registration - &lt;a href="https://www.w3.org/TR/webauthn/images/webauthn-registration-flow-01.svg">www.w3.org&lt;/a>&lt;/p>
&lt;p>To summarise, the above flow registers a new instance of an authenticator. Then
it verifies that the same authenticator is bound to the account. That is done
using a unique public/private key pair where the private key is in the
authenticator. Note that the authenticator doesn&amp;rsquo;t map a particular user to an
account. That is done thru the other process flow and by the &lt;a href="https://www.w3.org/TR/webauthn/#webauthn-relying-party">relying
party&lt;/a>.&lt;/p>
&lt;p>The flow below shows how a registered authenticator is used to authenticate the
account holder.&lt;/p>
&lt;p>&lt;img src="https://www.w3.org/TR/webauthn/images/webauthn-authentication-flow-01.svg" alt="WebAuthn Authentication">&lt;/p>
&lt;p align = "center"> FIDO2 Authentication - &lt;a href="https://www.w3.org/TR/webauthn/images/webauthn-authentication-flow-01.svg">www.w3.org&lt;/a>&lt;/p>
&lt;p>The Command pattern was the perfect solution for the first authenticator
implementation because it supported all of our use cases, but same time was
simplest. Most straightforward to integration was naturally with a programming
language it was implemented which was Go.&lt;/p>
&lt;p>The second thing was to figure out how we would like to implement interprocess
communication. For that, the command pattern is suited very well. Fill the
command with all the needed data and give one of the operations we were
supporting: &lt;code>register&lt;/code> and &lt;code>login&lt;/code> from the FIDO2 standard. The process
communication is handled just as the process starts by reading the command from
JSON. That is suited for Node.js use as well. (For the record, my fantastic
colleague Laura did all the needed Node.js work.)&lt;/p>
&lt;p>When we considered security, we followed our post-compromise principle. We
didn&amp;rsquo;t (yet) try to solve the situation where someone managed to hack the server
and hook a debugger to our processes without our notice. To solve that, we need
&lt;a href="https://en.wikipedia.org/wiki/Trusted_execution_environment">TEE&lt;/a> or similar.
Our specification is ready, but before the implementation, we should think if
it&amp;rsquo;s worth it, and about the use case we are implementing.&lt;/p>
&lt;h3 id="stateless-authenticator">Stateless Authenticator&lt;/h3>
&lt;p>Because you rarely find anything that removes complexity from your
implementation from security-related standards or specifications, it&amp;rsquo;s forth of
mentioning: By following &lt;a href="https://www.w3.org/TR/webauthn/">WebAuthn
specification&lt;/a> I did learn that I could, once
again, use crypto chaining!&lt;/p>
&lt;p>We knew that you would use one authenticator for many different places. That was
clear, of course. But when an authenticator is used for the service or as a
service, there is the next tenancy level.&lt;/p>
&lt;p>Before I started to write the whole thing, I thought that I use our server-side
secure enclave to store all the key pairs there and let the tenant set the
enclave&amp;rsquo;s master key. It would still mean that the implementation would be
state-full. From the operations' perspective, we all know what that means: more
things to take care of and manage, but most importantly, one potential
scalability issue to solve.&lt;/p>
&lt;p>The FIDO2 standard documentation describes a perfect solution for our needs
which made our authenticator stateless. You give the other party your public
key, but you give your private key in your &lt;code>credential ID&lt;/code>. It might
first sound crazy, but it&amp;rsquo;s genius indeed.&lt;/p>
&lt;p>Hold on! That cannot be?&lt;/p>
&lt;p>But it is. You have to build your identifier to include your private key, but no
one but you can use it because you have encrypted it with a symmetric master
key. The key that no one but you controls.&lt;/p>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 1301px">
&lt;img class="card-img-top" src="/blog/2021/11/09/anchoring-chains-of-trust/MasterKeyMain_hu010c6f71b85452493ec3cbb36ce48ca0_2698229_1291x0_resize_catmullrom_3.png" width="1291" height="663">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>Stateless Authenticator Implementation&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>The draft above illustrates how our stateless FIDO2 authenticator works at
a master key level. Other factors like a cloning counter and an authenticator ID
are left out for simplicity.&lt;/p>
&lt;ol>
&lt;li>We can ask TEE to create a new key pair for FIDO2 registration, which gives
us a unique key pair that includes &lt;code>public key&lt;/code> and &lt;em>encrypted private key&lt;/em>,
i.e. &lt;code>credential ID&lt;/code>.&lt;/li>
&lt;li>Our authenticator sends the FIDO2 &lt;code>attestation object&lt;/code> to the server.&lt;/li>
&lt;li>When the authenticator receives the FIDO2 challenge during authentication, it builds
it to the key pair in the same format as registration.&lt;/li>
&lt;li>The TEE inside the authenticator builds us the &lt;code>assertion object&lt;/code> ready to send
to the FIDO2 server.&lt;/li>
&lt;/ol>
&lt;p>As we can see, the master key never leaves the TEE. The implementation can
be done with help cloud
&lt;a href="https://en.wikipedia.org/wiki/Hardware_security_module">HSM&lt;/a> or
&lt;a href="https://www.neclab.eu/research-areas/security/nec-labs-introduce-a-new-solution-enables-seamless-provisioning-and-decommissioning-of-tee-based-applications-in-the-cloud">TEE-based app&lt;/a>;
or we can implement an application with the help of &lt;a href="https://aws.amazon.com/ec2/nitro/nitro-enclaves/">AWS
Nitro Enclaves&lt;/a> or similar.&lt;/p>
&lt;p>&lt;strong>Note!&lt;/strong> This is not a good solution for a pure client-side &lt;em>software-based&lt;/em>
authenticator, because it needs help from the hardware, i.e. secure enclave.
It&amp;rsquo;s suitable for &lt;em>hardware-based and certain types of server-side solutions&lt;/em>
where you can use TEE or similar solutions.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>FIDO2 authentication is an excellent match for DID Wallet authentication. gRPC
transport combined with JWT authorization has been straightforward to use. Our
gRPC SDK allows you to implicitly move the JWT token during the API calls after
opening the server connection. Plus, gRPC&amp;rsquo;s capability to have bidirectional
streams make the programming experience very pleasant. Finally, an option is to
authenticate the gRPC connection between server and client with (no PKI is
needed) TLS certificates: You can authorize software components to bind to your
deployment.&lt;/p>
&lt;p>The SDK and the API we have built with this stack have fulfilled all our
expectations:&lt;/p>
&lt;ul>
&lt;li>security&lt;/li>
&lt;li>performance&lt;/li>
&lt;li>easy to use&lt;/li>
&lt;li>solving &lt;a href="https://en.wikipedia.org/wiki/Don%27t_repeat_yourself">DRY&lt;/a> e.g.
error handling&lt;/li>
&lt;li>polyglot&lt;/li>
&lt;li>cloud-ready&lt;/li>
&lt;li>micro-service friendly&lt;/li>
&lt;/ul>
&lt;p>And hopefully yours. Give it a try!&lt;/p></description></item><item><title>Blog: The Arm Adventure on Docker</title><link>/blog/2021/09/20/the-arm-adventure-on-docker/</link><pubDate>Mon, 20 Sep 2021 00:00:00 +0000</pubDate><guid>/blog/2021/09/20/the-arm-adventure-on-docker/</guid><description>
&lt;p>Since the Findy Agency project launched, Docker has been one of our main tools to help set up the agency development and deployment environments. First of all, we use Docker images for our cloud deployment. On a new release, the CI build pipeline bundles all needed binaries to each service image. After the build, the pipeline pushes Docker images to &lt;a href="https://github.blog/2020-09-01-introducing-github-container-registry/">the GitHub container registry&lt;/a>, from where the deployment pipeline fetches them and updates the cloud environment.&lt;/p>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 706px">
&lt;img class="card-img-top" src="/blog/2021/09/20/the-arm-adventure-on-docker/cover-agency-deployment-pipeline_hu45c26a180b4a9f8ed9e980905a894a35_52542_925x925_fit_catmullrom_3.png" width="696" height="385">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>Agency deployment pipeline: New release triggers image build in GitHub actions. When the new container image is in the registry, AWS Code Pipelines handles the deployment environment update.&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>In addition, we’ve used Docker to take care of the service orchestration in a local development environment. When developing the agency itself, a native setup with full debugging capabilities is naturally our primary choice. However, suppose one wishes only to use the agency services and develop, e.g., a web service utilizing agency capabilities. In that case, the most straightforward approach is to run agency containers with a preconfigured docker-compose script. The script pulls correct images to the local desktop and sets needed configuration parameters. Setting up and updating the three services could be cumbersome without the orchestration, at least for newbies.&lt;/p>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 635px">
&lt;img class="card-img-top" src="/blog/2021/09/20/the-arm-adventure-on-docker/arch-overview_hud3b603a9121f1582ad6844475842d5cd_90814_625x625_fit_catmullrom_3.png" width="625" height="546">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>High-level architecture of Findy Agency. Setting up the agency to localhost is most straightforward with the help of a container orchestration tool.&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>Until recently, we were happy with our image-building pipeline. Local environments booted up with ease, and the deployment pipeline rolled out updates beautifully. Then one day, our colleague with an M1-equipped Mac tried out the docker-compose script. Running the agency images in an Arm-based architecture was something we hadn&amp;rsquo;t considered. We built our Docker images for amd64 architecture, while M1 Macs expect container images for arm64 CPU architecture. It became clear we needed to support also the arm64, as we knew that the popularity of the M1 chipped computers would only increase in the future.&lt;/p>
&lt;h2 id="multi-architecture-support-in-docker">Multi-architecture Support in Docker&lt;/h2>
&lt;p>Typically, when building images for Docker, the image inherits the architecture type from the building machine. And as each processor architecture requires a dedicated Docker image, one needs to build a different container image for each target architecture. To avoid the hassle with the multiple images, Docker has added support for multi-architecture images. It means that there is a single image in the registry, but it can have many variants. Docker will automatically choose the appropriate architecture for the processor and platform in question and pull the correct variant.&lt;/p>
&lt;p>Ok, so Docker takes care of the image selection when running images. How about building them then? There are &lt;a href="https://docs.docker.com/buildx/working-with-buildx/#build-multi-platform-images">three strategies&lt;/a>.&lt;/p>
&lt;ol>
&lt;li>&lt;strong>QEMU emulation support in the kernel&lt;/strong>: QEMU works by emulating all instructions of a foreign CPU instruction set on the host processor. For example, it can emulate ARM CPU instructions on an x86 host machine, and thus, the QEMU emulator enables building images that target another architecture than the host. This approach usually requires the fewest modifications to the existing Dockerfiles, but the build time is the slowest.&lt;/li>
&lt;li>&lt;strong>Multiple native nodes using the same builder instance&lt;/strong>: Hosts with different CPU architectures execute the build. The build time is faster than with the other two alternatives. The drawback is that it requires access to as many native nodes as there are target architectures.&lt;/li>
&lt;li>&lt;strong>Stage in a Dockerfile for cross-compilation&lt;/strong>: This option is possible with languages that support cross-compilation. Arguments exposing the build and the target platforms are automatically available to the build stage. The build command can utilize these parameters to build the binary for the correct target. The drawback is that the builder needs to modify the Dockerfile build commands and perhaps familiarize oneself with using the cross-compilation tools for the target language.&lt;/li>
&lt;/ol>
&lt;p>From these three options, we chose the first one, as it seemed the most straightforward route. However, in our case, the third option might have worked as well since we are building with tools that support cross-compilation, Rust, and GoLang.&lt;/p>
&lt;p>A Docker CLI plugin, &lt;a href="https://docs.docker.com/buildx/working-with-buildx/">buildx&lt;/a>, is required to build multi-architecture images. It extends the docker command with additional features, the multi-architecture build capability being one of them. Using buildx is almost the same as using the ordinary Docker build function. The target platform is added to the command with the flag &lt;code>--platform&lt;/code>.&lt;/p>
&lt;p>Example of building Docker image with buildx for arm64:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">docker buildx build --platform linux/arm64 -t image_label .
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="reviewing-the-build-receipts">Reviewing the Build Receipts&lt;/h2>
&lt;p>Now we had chosen the strategy and had the tools installed. The next step was to review each image stack and ensure that it was possible to build all image layers for the needed variants.&lt;/p>
&lt;p>Our default image stack consists of &lt;a href="https://github.com/findy-network/findy-common-go/blob/master/infra/aws/Dockerfile.indy.ubuntu">the custom base image&lt;/a> and an application layer (service binary in question). The custom base image contains some tools and libraries that are common for all of our services. It expands the official Docker image for Ubuntu.&lt;/p>
&lt;p>For the official Docker images, there are no problems since Docker provides the needed variants out-of-the-shelf. However, our custom base image installs indy-sdk libraries from the Sovrin Debian repository, and unfortunately, the Debian repository did not provide binaries for arm64. So instead of installing the library from the Debian repository, we needed to add &lt;a href="https://github.com/findy-network/findy-common-go/blob/8bef1cbc4cc7d698275a69a9c9c4aff2622b84de/infra/aws/Dockerfile.indy.ubuntu#L12">a build step&lt;/a> that would build and install the indy-sdk from the sources. Otherwise, building for arm64 revealed no problems.&lt;/p>
&lt;h2 id="integration-to-github-actions">Integration to GitHub Actions&lt;/h2>
&lt;p>The final step was to modify our GitHub Actions pipelines to build the images for the different architectures. Fortunately, Docker provides ready-made actions for setting up QEMU (&lt;em>&lt;a href="https://github.com/docker/setup-qemu-action">setup-qemu-action&lt;/a>&lt;/em>) and buildx (&lt;em>&lt;a href="https://github.com/docker/setup-buildx-action">setup-buildx-action&lt;/a>&lt;/em>), logging to the Docker registry (&lt;em>&lt;a href="https://github.com/docker/login-action">login-action&lt;/a>&lt;/em>), and building and pushing the ready images to the registry (&lt;em>&lt;a href="https://github.com/docker/build-push-action">build-push-action&lt;/a>&lt;/em>).&lt;/p>
&lt;p>We utilized the actions provided by Docker, and &lt;a href="https://github.com/findy-network/findy-agent/blob/master/.github/workflows/release.yml">the release workflow&lt;/a> for findy-agent looks now like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yml" data-lang="yml">&lt;span style="color:#204a87;font-weight:bold">name&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">release&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">on&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">push&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">tags&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>- &lt;span style="color:#4e9a06">&amp;#39;*&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">jobs&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">push-image&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">runs-on&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">ubuntu-latest&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">permissions&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">packages&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">write&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">contents&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">read&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">steps&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>- &lt;span style="color:#204a87;font-weight:bold">uses&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">actions/checkout@v2&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>- &lt;span style="color:#204a87;font-weight:bold">name&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">Set up QEMU&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">uses&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">docker/setup-qemu-action@v1&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">with&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">platforms&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">all&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>- &lt;span style="color:#204a87;font-weight:bold">name&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">Set up Docker Buildx&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">uses&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">docker/setup-buildx-action@v1&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>- &lt;span style="color:#204a87;font-weight:bold">name&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">Login to Registry&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">uses&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">docker/login-action@v1&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">with&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">registry&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">ghcr.io&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">username&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">${{ github.repository_owner }}&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">password&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">${{ secrets.GITHUB_TOKEN }}&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>- &lt;span style="color:#204a87;font-weight:bold">run&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">echo &amp;#34;version=$(cat ./VERSION)&amp;#34; &amp;gt;&amp;gt; $GITHUB_ENV&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>- &lt;span style="color:#204a87;font-weight:bold">uses&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">docker/build-push-action@v2&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">with&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">platforms&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">linux/amd64,linux/arm64&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">push&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">true&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">tags&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000;font-weight:bold">|&lt;/span>&lt;span style="color:#8f5902;font-style:italic">
&lt;/span>&lt;span style="color:#8f5902;font-style:italic"> ghcr.io/${{ github.repository_owner }}/findy-agent:${{ env.version }}
&lt;/span>&lt;span style="color:#8f5902;font-style:italic"> ghcr.io/${{ github.repository_owner }}/findy-agent:latest&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">cache-from&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">type=registry,ref=ghcr.io/${{ github.repository_owner }}/findy-agent:latest&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">cache-to&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">type=inline&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">file&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#000">./scripts/deploy/Dockerfile&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The result was as expected; the actions took care of building of the container images successfully. The build process is considerably slower with QEMU, but luckily, the build caches speed up the process.&lt;/p>
&lt;p>Now have the needed variants for our service images in the registry. Furthermore, our colleague with the M1-Mac can run the agency successfully with his desktop.&lt;/p>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 935px">
&lt;img class="card-img-top" src="/blog/2021/09/20/the-arm-adventure-on-docker/findy-agent-packages_huee06773afc479fec8fbdb4f9dc73d730_149630_925x925_fit_catmullrom_3.png" width="925" height="483">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>&lt;a href="https://github.com/findy-network/findy-agent/pkgs/container/findy-agent">Docker registry&lt;/a> for Findy agent&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div></description></item><item><title>Blog: Travelogue</title><link>/blog/2021/09/08/travelogue/</link><pubDate>Wed, 08 Sep 2021 00:00:00 +0000</pubDate><guid>/blog/2021/09/08/travelogue/</guid><description>
&lt;p>The success of our team is measured:&lt;/p>
&lt;ul>
&lt;li>How well do we understand &lt;em>certain&lt;/em> emerging technologies?&lt;/li>
&lt;li>How relevant they are to the business we are in?&lt;/li>
&lt;li>How much potential do they have for our company&amp;rsquo;s business?&lt;/li>
&lt;/ul>
&lt;p>If you are asking yourself if the order of the list is wrong, the answer is, it
is not.&lt;/p>
&lt;p>We have learned that you will fail if you prioritize technologies by their
business value too early. There is a catch, though. You must be sure that you
will not fall in love with the technologies you are studying. Certain scepticism
is welcomed in our line of work. That attitude may follow thru this post as
well. You have now been warned, at least.&lt;/p>
&lt;h3 id="technology-tree">Technology Tree&lt;/h3>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 2001px">
&lt;img class="card-img-top" src="/blog/2021/09/08/travelogue/TechTree_hucb3e947ff7ac3c644fea522c44e98b42_1584631_1991x0_resize_catmullrom_3.png" width="1991" height="1120">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>Findy Agency Tech Tree&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>Technology roots present the most important &lt;em>background knowledge&lt;/em> of our study.
The most fundamental technologies and study subjects are in the trunk. The
trunk is the backbone of our work. It ties it all together. Branches and leaves
are outcomes, conclusions, and key learnings. At the top of the tree, some
future topics are considered but not implemented or even tried yet.&lt;/p>
&lt;p>Even if the technology tree illustrates the relevant technologies for the
subject, we will not address them in this post. &lt;strong>We recommend you to study the
tree for a moment&lt;/strong> to get the picture. You will notice that there aren&amp;rsquo;t any
mention of VC. For us, the concept of VC is an internal feature of
&lt;a href="https://www.w3.org/TR/vc-data-model/">DID system&lt;/a>. Naturally, there are a huge
amount of enormous study subjects inside VCs like ZKP, etc. But this approach
has to lead us to concentrate on the network itself and the structure it should
have.&lt;/p>
&lt;p>The tree has helped us to see how things are bound together and what topics are
the most relevant for the study area.&lt;/p>
&lt;h3 id="trust-triangle">Trust Triangle&lt;/h3>
&lt;p>The famous SSI trust triangle is an excellent tool to simplify what is
important and what is not. As we can see, everything builds on peer to peer
connections, thick arrows. VCs are issued, and proofs are presented thru them.
The only thing that&amp;rsquo;s not yet solved &lt;em>at the technology level&lt;/em> is the trust
arrow in the triangle. (I know the recursive trust triangle, but I disagree with
how it&amp;rsquo;s thought to be implemented). But &lt;em>this blog post&lt;/em> is not about that
either.&lt;/p>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 605px">
&lt;img class="card-img-top" src="/blog/2021/09/08/travelogue/trust-triangle_hua2e42792a9d20037c5f572b0412e67c1_57626_925x925_fit_catmullrom_3.png" width="595" height="392">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>The SSI Trust Triangle&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;h3 id="targets-and-goals">Targets and Goals&lt;/h3>
&lt;p>Every project&amp;rsquo;s objectives and goals change during the execution. The longer the
project, the more pivots it has. (Note that I use the term &lt;em>project&lt;/em> quite
freely in the post). When we started to work with DID/SSI field, the goal was to
build &lt;strong>a standalone mobile app demo&lt;/strong> of the identity wallet based on
Hyperledger Indy. We started in &lt;em>test drive mode&lt;/em> but built a full DID agency
and published it as OSS. The journey has been inspiring, and we have learned a
lot.&lt;/p>
&lt;p>In every project, it&amp;rsquo;s important to maintain the scope. Thanks to the nature of
our organisation we didn&amp;rsquo;t have changing requirements. The widening of the scope
came mostly from the fact that the whole area of SSI and DID were evolving. It
still is.&lt;/p>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 935px">
&lt;img class="card-img-top" src="/blog/2021/09/08/travelogue/targets_hu3c26761ab0ee4aee9c5a7a0b4088e778_867526_925x925_fit_catmullrom_3.png" width="925" height="520">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>The Journey From Identity Wallet to Identity Ecosystem&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>Many project goals changed significantly during the execution, but that was part
of the job. And as DID/SSI matured, we matured as well, and goals to our work
aren&amp;rsquo;t &lt;em>so&lt;/em> &lt;em>test-driven&lt;/em> mode anymore. We still test other related technologies
that can be matched to DID/SSI or even replace some parts of it
but have transited to the state where we have started to build our own related
core technologies to the field.&lt;/p>
&lt;p>Incubators usually start their trip by testing different hypotheses and trying
them out in practice. We did the same but more on the practical side. We didn&amp;rsquo;t
have a formal hypothesis, but we had some use cases and a vision of how modern
architecture should work and its scaling requirements. Those kinds of
principles lead our &lt;em>decision-making process&lt;/em> during the project.
(Maybe some of us write a detailed blog about how our emerging tech process and
organisation worked.)&lt;/p>
&lt;h2 id="the-journey">The journey&lt;/h2>
&lt;p>We have been building our multi-tenant agency since the beginning of 2019.
During that time, we have tried many different techniques, technologies and
architectures, and application metaphors. We think we have succeeded to find
interesting results.&lt;/p>
&lt;p>In the following chapters, we will report the time period from the beginning of
2019 to autumn of 2021 in half of a year intervals. I really recommend that you
look at the timelines carefully because they include valuable outcomes.&lt;/p>
&lt;h3 id="2019h1">2019/H1&lt;/h3>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 2010px">
&lt;img class="card-img-top" src="/blog/2021/09/08/travelogue/spring19_hu29db0a4a24cce1cc4a7a017360286d3b_559718_2000x0_resize_catmullrom_3.png" width="2000" height="1125">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>2019/H1&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;h5 id="the-start">The Start&lt;/h5>
&lt;p>Me:&lt;/p>
&lt;blockquote>
&lt;p>&amp;ldquo;I&amp;rsquo;m interested in studying new tech by programming with it.&amp;rdquo;&lt;/p>
&lt;/blockquote>
&lt;p>Some block-chain experts in the emerging technologies team:&lt;/p>
&lt;blockquote>
&lt;p>&amp;ldquo;We need an identity wallet to be able to continue with our other projects.
Have you ever heard of Hyperledger Indy..&amp;rdquo;&lt;/p>
&lt;/blockquote>
&lt;p>In one week, I have been smoke-tested indy SDK on iOS and Linux. During the
spring, we ended up following the Indy&amp;rsquo;s proprietary agent to agent
protocol, &lt;strong>but&lt;/strong> we didn&amp;rsquo;t use &lt;em>libcvx&lt;/em> for that because:&lt;/p>
&lt;blockquote>
&lt;p>This library is currently in an &lt;strong>experimental&lt;/strong> state and is not part of
official releases. - [indy SDK GitHub pages]&lt;/p>
&lt;/blockquote>
&lt;p>To be honest, that was the most important reason because we have had so much
extra work with other Indy libs, and of course, we would need a wrapper at least for
Go. It was an easy decision. Afterwards, it was the right because the DIDComm
protocol is the backbone of everything with SSI/DID. And now, when it&amp;rsquo;s in our
own (capable) hands, it&amp;rsquo;s made many such things possible which weren&amp;rsquo;t otherwise.
We will publish a whole new technical blog series of our multi-tenant DIDComm
protocol engine.&lt;/p>
&lt;p>All of the modern, native mobile apps end up been written from two parts: the mobile
app component running on the device and the server part doing everything it can
to make the mobile app&amp;rsquo;s life easier. Early stages, DIDComm&amp;rsquo;s edge and cloud agent
roles weren&amp;rsquo;t that straightforward. From every point of view, it seemed overly
complicated. But still, we stuck to it.&lt;/p>
&lt;h5 id="first-results">First Results&lt;/h5>
&lt;p>At the end of spring 2019, we had a quick and dirty demo of the system, which had
&lt;strong>multi-tenant&lt;/strong> agency to serve cloud agents and iOS mobile app to run edge
agents. An EA onboarded itself to the agency with the same DID Connect
protocol, which was used everywhere. Actually, an EA and a CA
used Web Sockets as a transport mechanism for indy&amp;rsquo;s DIDComm messages.&lt;/p>
&lt;p>We hated the protocol. It was insane. But it was DID protocol, wasn&amp;rsquo;t it?&lt;/p>
&lt;p>The system was end to end encrypted, but the indy protocol had its flaws, like
being synchronous. We didn&amp;rsquo;t yet have any persistent state machine or the other
basics of the communication protocol systems. Also, the whole thing felted
overly complicated and old &amp;ndash; it wasn&amp;rsquo;t modern cloud protocol.&lt;/p>
&lt;h5 id="third-party-integration-demo">Third party integration demo&lt;/h5>
&lt;p>In early summer ended up building a demo that didn&amp;rsquo;t follow totally the current
architecture, because the mobile app&amp;rsquo;s edge agent was communicating directly to
the third party agent. This gave us a lot of experience, and for me, it gave
needed time to spec what kind of protocol the DIDComm should be and what kind of
protocol engine should run it.&lt;/p>
&lt;p>It was a weird time because indy&amp;rsquo;s legacy agent to agent protocol didn&amp;rsquo;t have a
public, structured and formal specification of its protocol.&lt;/p>
&lt;p>Those who are interested in history can read more from
&lt;a href="https://hyperledger-indy.readthedocs.io/projects/hipe/en/latest/text/0002-agents/README.html">here&lt;/a>.&lt;/p>
&lt;p>The integration project made it pretty clear for us what kind of protocol was
needed.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Async with explicit state machine&lt;/strong>&lt;/p>
&lt;/blockquote>
&lt;p>DIDComm must be async and message-driven simple because it&amp;rsquo;s deliberative in
its nature. Two agents are negotiating for issuing, proofing, etc.&lt;/p>
&lt;h5 id="aries-news">Aries news&lt;/h5>
&lt;p>Hyperledger Aries was set up during the summer, which was good because it showed
the same we learned. We were on the right path.&lt;/p>
&lt;h5 id="code-delivery-for-a-business-partner">Code Delivery For a Business Partner&lt;/h5>
&lt;p>For this mail-stone, we ended up producing some documentation, mostly to
explain the architecture. During the whole project, we have had a comprehensive
unit and integration test harness.&lt;/p>
&lt;p>At this point, we had all of the important features covered: issuing, holding,
present and verify proofs in a quick and dirty way. Now we knew the potential.&lt;/p>
&lt;h5 id="results-2019-summer">Results 2019 Summer&lt;/h5>
&lt;p>We had managed to implement pre-Aries DIDComm over HTTP and WebSocket. We had
a multi-tenant agency running cloud agents even though it was far from
production readiness. Everything was end to end encrypted. The current agency
supported indy&amp;rsquo;s ledger transactions, and first, we had taken some tests from
issuing and proofing protocols. We started to understand what kind of beast was
tearing at us from another end of the road.&lt;/p>
&lt;h3 id="2019h2">2019/H2&lt;/h3>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 2010px">
&lt;img class="card-img-top" src="/blog/2021/09/08/travelogue/autumn19_hu29db0a4a24cce1cc4a7a017360286d3b_515478_2000x0_resize_catmullrom_3.png" width="2000" height="1125">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>2019/H2&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;h5 id="start-of-the-async-protocol-development">Start Of The Async Protocol Development&lt;/h5>
&lt;p>When we started architecture redesign after the summer break,
we had a clear idea of what kind of direction we should take and what to leave for
later:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Cloud-first&lt;/strong> and we have newer wanted to step back on that.&lt;/li>
&lt;li>&lt;strong>Modern micro-service architecture&lt;/strong> targeting continuous delivery and
scalability. That leads to a certain type of technology stack which consists of
techs like Go, gRPC, Docker (or other containerization technology), container
orchestration like K8s, etc. One key requirement was that hardware utilization
must be perfect, i.e. tiny servers are enough.&lt;/li>
&lt;li>&lt;strong>No support for offline&lt;/strong> use cases &lt;em>for now&lt;/em>.&lt;/li>
&lt;li>&lt;strong>No revocation&lt;/strong> until there is a working solution. Credit card revocation has
taught us a lot. Scalable and fast revocation is a hard problem to solve.&lt;/li>
&lt;li>Message routing should not be part of the protocol&amp;rsquo;s explicit &amp;lsquo;headers&amp;rsquo;, i.e.
there is &lt;strong>only one service endpoint for a DID&lt;/strong>. We should naturally handle the
service endpoint so that privacy is maintained as it is in our agency. By
leaving routing out, it has been making everything so much simple. Some
technologies can do that for us for free, like Tor. We have tested Tor, and it
works pretty well for setting service endpoints and also connecting to them.&lt;/li>
&lt;li>&lt;strong>Use push notifications along with the WebSockets&lt;/strong>, i.e. lent APNS to trigger
edge agents when they were not connected to the server.&lt;/li>
&lt;/ul>
&lt;h5 id="multi-ledger-architecture">Multi-ledger Architecture&lt;/h5>
&lt;p>Because everything goes through our Go wrapper to the Plenum ledger, I made a
version that used memory or plain file instead of the ledger as a hobby project.
It was meant to be used only for tests and development. Later the plug-in
architecture has allowed us to have other persistent saving media as well. But
more importantly, it has helped development and automatic testing a lot.&lt;/p>
&lt;p>Technically the &lt;em>hack&lt;/em> is to use &lt;code>pool handle&lt;/code> to tell if the system is
connected to a ledger or some other predefined media. &lt;code>indy&lt;/code> API has only two
functions that take &lt;code>pool handle&lt;/code> as an argument but doesn&amp;rsquo;t use it at all &lt;em>or&lt;/em>
a handle is an option.&lt;/p>
&lt;h5 id="server-side-secure-enclaves">Server Side Secure Enclaves&lt;/h5>
&lt;p>During the server-side development, we wanted to have at least post-compromised
secured key storage for cloud servers. Cloud environments like AWS give you
managed storage for master secrets, but we needed more when developing OSS
solutions with high performance and scalability requirements.&lt;/p>
&lt;p>Now we store our most important keys for LMDB-based fast key-value storage
fully encrypted. Master keys for installation are in a managed cloud environments
like AWS, Google, Azure, etc.&lt;/p>
&lt;h5 id="first-multi-tenant-chat-bot">First Multi-tenant Chat Bot&lt;/h5>
&lt;p>The first running version of the chatbot used a semi-hard-coded version.
It supported only sequential steps: a single line in a text file,
&lt;code>CredDefIds&lt;/code> in its own file, and finally text messages in its own files. The
result was just a few lines of Go code, thanks to its concurrent model.&lt;/p>
&lt;p>The result was so good that I made a backlog issue to start studying to
use SCXML or some other exciting language for chatbot state machines later.
About a year later, I implemented a state machine on my own with a proprietary YAML
format.&lt;/p>
&lt;p>But that search isn&amp;rsquo;t totally over. Before that, I considered many different
options, but there wasn&amp;rsquo;t much of an OSS alternative. One option could be to
embed Lua combined with the current state machine engine and replace the memory
model with Lua. We shall see what the real use case needs are.&lt;/p>
&lt;p>I personally think that an even more important approach would be &lt;strong>a state machine
verifier&lt;/strong>. Keeping that as a goal sets strict limits to the computation
model we could use. What we have learned now is you don&amp;rsquo;t need the full power of
general programming language but
&lt;a href="https://en.wikipedia.org/wiki/Automata_theory">finite state machine (automata theory)&lt;/a>
could just be enough.&lt;/p>
&lt;h4 id="2019h2-results">2019/H2 Results&lt;/h4>
&lt;p>We had implemented all the most important use cases with our new protocol
engine. We had an symmetric agent which could be in all of the needed roles of SSI:
a holder, an issuer, and a verifier. Also, the API seemed to be OK at a high
level of abstraction. The individual messages were shit.&lt;/p>
&lt;p>At the end of the year, we also had a decent toolbox both on command-line and
especially on the web.&lt;/p>
&lt;h3 id="2020h1">2020/H1&lt;/h3>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 2010px">
&lt;img class="card-img-top" src="/blog/2021/09/08/travelogue/spring20_hu29db0a4a24cce1cc4a7a017360286d3b_486983_2000x0_resize_catmullrom_3.png" width="2000" height="1125">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>2020/H1&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;h5 id="findy-consortium-level-oss-publication">Findy-consortium Level OSS Publication&lt;/h5>
&lt;p>At the beginning of 2020, we decided to publish all the produced
code inside the Findy consortium. We produced the new GitHub account, and code
without history moved from original repos to new ones.&lt;/p>
&lt;p>Even the decision brought a lot of routine work for that moment, it also brought
many good things:&lt;/p>
&lt;ul>
&lt;li>refactoring,&lt;/li>
&lt;li>interface cleanups,&lt;/li>
&lt;li>documentation updates.&lt;/li>
&lt;/ul>
&lt;h5 id="aca-py-interoperability-tests">ACA-Py Interoperability Tests&lt;/h5>
&lt;p>We implemented the first version of the new async protocol engine with existing
JSON messages came from legacy indy a2a protocols. It&amp;rsquo;s mostly because I
wanted to build it in small steps, and it worked pretty well.&lt;/p>
&lt;p>Most of the extra work did come from the legacy API we had. JSON messages over
indy&amp;rsquo;s proprietary DIDComm. As always, some bad but some good: because we had to
keep both DIDComm message formats, I managed to integrate a clever way to
separate different formats and still generalise with Go&amp;rsquo;s interfaces.&lt;/p>
&lt;h5 id="new-cli">New CLI&lt;/h5>
&lt;p>We noticed that Agency&amp;rsquo;s command-line UI started to be too complicated. Go has
a clever idea of how you can do services without environmental variables. I&amp;rsquo;m
still the guy who would stick with that, but it was a good idea to widen the
scope to make our tools comfortable for all new users.&lt;/p>
&lt;p>Our design idea was to build CLI, which follows subcommands like git and docker
nowadays. The latest version we have now is quite good already, but the road was
rocky. It is not easy to find the right structure the first time. The more you
use your CLI by yourself, the more you start to notice what is intuitive and
what is not.&lt;/p>
&lt;p>We decided to separate CLI commands from Agency to own tool and git repo. It was
good to move for that time, and when we managed to make it right, we were able to
move those same commands pack to the agency one year later because we needed CLI
tool without any &lt;em>libindy&lt;/em> dependencies. That is a good example of successful
software architecture work. You cannot predict the future, but you can prepare
yourself for change.&lt;/p>
&lt;h3 id="2020h2">2020/H2&lt;/h3>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 2010px">
&lt;img class="card-img-top" src="/blog/2021/09/08/travelogue/autumn20_hu29db0a4a24cce1cc4a7a017360286d3b_544015_2000x0_resize_catmullrom_3.png" width="2000" height="1125">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>2020/H2&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;h5 id="architecture-planning">Architecture Planning&lt;/h5>
&lt;p>I had had quite a long time the idea of using gRPC for the Cloud Agent
controller. My core idea was to get rid of the EA because, currently, it was
just an onboarding tool. The wallet had, included only the pairwise DID to its
cloud agent, nothing else. The actual wallet (we called it worker edge agent
wallet) was the real wallet, where the VCs were. I went thru
many similar protocols until I found FIDO UAF. The protocol is similar to
DIDComm&amp;rsquo;s pairwise protocol, but it&amp;rsquo;s not symmetric. Another end is the server,
and the other has the authenticator &amp;ndash; the cryptographical root of
trust.&lt;/p>
&lt;p>When I presented an internal demo of the gRPC with the JWT authorization and
explained that authentications would be FIDO2 WebAuthn, we were ready to start
the architecture transition. Everything was still good when I implemented the
first FIDO server with the help of Duo Labs Go packages. Our FIDO2 server was
now capable of allocating cloud agents. But there was one missing part I was
hoping someone in the OSS community would implement until we needed it. It was a
headless WebAuthn/UAF authenticator for those issuers/verifiers running as
service agents. How to onboard them, and how they would access the agency&amp;rsquo;s
resources with the same JWT authorization? To allow us to proceed, we added
support to get JWT by our old API. It&amp;rsquo;s was only intermediated solution but
served its purpose.&lt;/p>
&lt;h5 id="learnings-when-implementing-the-new-architecture">Learnings when implementing the new architecture&lt;/h5>
&lt;ul>
&lt;li>implicit JWT authorization helps gRPC usage a lot and simplifies it too.&lt;/li>
&lt;li>gRPC streams and Go&amp;rsquo;s channel is just excellent together.&lt;/li>
&lt;li>You should use pre-generated wallet keys for indy wallets.&lt;/li>
&lt;li>We can integrate performance and scalability tests into CI.&lt;/li>
&lt;li>gRPC integration and unit testing could be done in the same way as with HTTP
stack in Go, i.e. inside a single process that can play both client and server.&lt;/li>
&lt;/ul>
&lt;h5 id="highlights-of-the-end-of-the-year-2020">Highlights of the end of the year 2020&lt;/h5>
&lt;p>We started to build for new SA architecture and allowed both our APIs to
existing. WebAuth server, headless authenticator, and Vault first versions were
now ready. Also, I did the first version of a state machine for service agent
implementation. We had an option to use immuDB instead of Plenum ledger.&lt;/p>
&lt;h3 id="2021h1">2021/H1&lt;/h3>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 2010px">
&lt;img class="card-img-top" src="/blog/2021/09/08/travelogue/spring21_hu29db0a4a24cce1cc4a7a017360286d3b_511190_2000x0_resize_catmullrom_3.png" width="2000" height="1125">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>2021/H1&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>Now we have an architecture that we can live with. All the important elements
are in place. Now we just clean it up.&lt;/p>
&lt;h5 id="summary-of-spring-2021-results">Summary of Spring 2021 Results&lt;/h5>
&lt;p>Until the summer, the most important results have been:&lt;/p>
&lt;ul>
&lt;li>Headless WebAuthn authenticator&lt;/li>
&lt;li>React-based Web Wallet&lt;/li>
&lt;li>Lots of documentation and examples&lt;/li>
&lt;li>Agency&amp;rsquo;s gRPC API v1&lt;/li>
&lt;li>Polyglot implementations gRPC: TypeScript, Go, JavaScript&lt;/li>
&lt;li>New toolbox both Web and Command-line&lt;/li>
&lt;li>The full OSS release&lt;/li>
&lt;/ul>
&lt;p>As said, all of the important elements are in place. However, our solution is
based on &lt;code>libindy&lt;/code>, which will be interesting because the Aries group moves to
shared libraries, whereas the original contributor continues with it. We haven&amp;rsquo;t
made the decision yet on which direction we will go. Or do we even need to
choose? At least in the meantime, we could add some new solutions and run them
both. Thanks to our own architecture and interfaces, those are plausible options
for our agency.&lt;/p>
&lt;p>There are many interesting study subjects we are continuing to work on within
SSI/DID. We will report them in upcoming blog posts. Stay tuned, folks!&lt;/p></description></item><item><title>Blog: Announcing Findy Agency</title><link>/blog/2021/08/11/announcing-findy-agency/</link><pubDate>Wed, 11 Aug 2021 00:00:00 +0000</pubDate><guid>/blog/2021/08/11/announcing-findy-agency/</guid><description>
&lt;p>Findy Agency provides a &lt;a href="https://www.hyperledger.org/use/aries">Hyperledger Aries&lt;/a> compatible identity agent service. It includes a web wallet for individuals and an API for organizations to utilize functionality related to verified data exchange: issuing, holding, verifying, and proving credentials. The agents hosted by the agency operate using DIDComm messaging and &lt;a href="https://github.com/hyperledger/aries-rfcs">Hyperledger Aries protocols&lt;/a>. Thus it is interoperable with other Hyperledger Aries compatible agents. The supported verified credential format is currently &lt;a href="https://github.com/hyperledger/indy-sdk">Hyperledger Indy&lt;/a> “Anoncreds” that work with Hyperledger Indy distributed ledger. However, the plan is to add more credential formats in the future.&lt;/p>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 935px">
&lt;img class="card-img-top" src="/blog/2021/08/11/announcing-findy-agency/vision_hu68846d3af499a7706e2100471d9aa9d5_34254_925x925_fit_catmullrom_3.png" width="925" height="322">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>Main design principles of Findy Agency&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>In this post, we share some background information on the project. If you want to skip the history, start directly with &lt;a href="/docs">the documentation&lt;/a> or &lt;a href="https://github.com/findy-network/findy-wallet-pwa/tree/dev/tools/env#agency-setup-for-local-development">launch Findy Agency in your local computer&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Verified data exchange as digitalization enabler&lt;/strong>&lt;/p>
&lt;p>Distributed and self-sovereign identity, along with verified data exchange between different parties, has been an area of our interest at &lt;a href="https://op-lab.fi/">the OP innovation unit&lt;/a> for quite some time. After all, when thinking about the next steps of digitalization, secure and privacy-preserving handling of identity is one of the main problem areas. When individuals and organizations can prove facts of themselves digitally, it will enable us to streamline and digitalize many processes that may be still cumbersome today, including those in the banking and insurance sectors.&lt;/p>
&lt;p>Since 2019 the Findy team at OP has been working on two fronts. We have collaborated with other Finnish organizations to set up a cooperative to govern &lt;a href="https://findy.fi/en/">a national identity network Findy&lt;/a>. At the same time, our developers have researched credential exchange technologies, concentrating heavily on Hyperledger Indy and Aries.&lt;/p>
&lt;p>&lt;strong>From scratch to success with incremental cycles&lt;/strong>&lt;/p>
&lt;p>When we started the development at the beginning of 2019, the verified credential world looked a whole lot different. Low-level indy-sdk was all that a developer had if wanting to work with indy credentials. It contained basic credential manipulation functionality but almost nothing usable related to communication between individuals or organizations. We were puzzled because the scenarios we had in mind involved users with mobile applications and organizations with web services and interaction happening between these two.&lt;/p>
&lt;p>Soon we realized that we needed to build all the missing components ourselves if we would want to do the experiments. And so, after multiple development cycles and as a result of these experiments became Findy Agency. The path to this publication has not always been straightforward: there have been complete refactorings and changes in the project direction along the way. However, we feel that now we have accomplished something that truly reflects our vision.&lt;/p>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 935px">
&lt;img class="card-img-top" src="/blog/2021/08/11/announcing-findy-agency/bot-scenario_hu15894898d496192ef1d77a0f852ae033_200283_925x925_fit_catmullrom_3.png" width="925" height="520">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;em>One of the team's experiments, Findy Bots, was built on Findy Agency. See demo video in &lt;a href="https://www.youtube.com/watch?v=gVr8KwISMS4">Youtube&lt;/a>.&lt;/em>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Why the hard way?&lt;/strong>&lt;/p>
&lt;p>The situation is not anymore so sad for developers wanting to add credential support to their app as it was three years ago. There are several service providers and even &lt;a href="https://github.com/hyperledger/aries#aries-agent-frameworks">open source solutions&lt;/a> one can choose from. So why did we choose the hard way and wrote an agency of our own? There are several reasons.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Experience&lt;/strong>: We believe that verified data technology will transform the internet in a profound way. It will have an impact on perhaps even the most important data processing systems in our society. We want to understand the technology thoroughly so that we know what we are betting on.&lt;/li>
&lt;li>&lt;strong>Open-source&lt;/strong>: As we stated in the previous bullet, we want to be able to read and understand the software we are running. In addition, community-given feedback and contributions improve the software quality. There is also a good chance that open-sourced software is more secure than proprietary since it has more eyes looking at the possible security flaws.&lt;/li>
&lt;li>&lt;strong>Pragmatic approach&lt;/strong>: We have scarce resources, so we have to concentrate on the most essential use cases. We do not wish to bloat the software with features that are far in the future if valid at all.&lt;/li>
&lt;li>&lt;strong>Performance&lt;/strong>: We aim to write performant software with the right tools for the job. We also value developer performance and hence have a special eye for the quality of the API.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>The Vision&lt;/strong>&lt;/p>
&lt;p>Our solution contains several features that make our vision and that we feel most of other open-source solutions are missing.&lt;/p>
&lt;p>Findy Agency has been &lt;strong>multi-tenant&lt;/strong> from the beginning of the project. It means single agency installation can securely serve multiple individuals and organizations without extra hassle.&lt;/p>
&lt;p>Agency architecture is based on &lt;strong>a cloud strategy&lt;/strong>. Credential data is stored securely in the cloud and cloud agents do all the credentials-related hard work on behalf of the agency users (wallet application/API users). The reasoning for the cloud strategy is that we think that individuals store their credential data rather with a confided service provider than worry about losing their device or setting up complex backup processes. Furthermore, the use cases relevant to us are also always executed online, so we have knowingly left out the logic aiming for offline scenarios. This enabled us to reduce the complexity related to mediator implementation.&lt;/p>
&lt;p>Due to the cloud strategy, we could drop out the requirement for the mobile application. Individuals can use &lt;strong>the web wallet&lt;/strong> with their device browser. Authentication to the web wallet is done with secure and passwordless &lt;strong>&lt;a href="https://webauthn.guide/">WebAuthn/FIDO protocol&lt;/a>&lt;/strong>.&lt;/p>
&lt;p>Agency backend services are implemented with &lt;strong>performance&lt;/strong> in mind. That is why we selected performant &lt;a href="https://golang.org/">GoLang&lt;/a> and &lt;a href="https://grpc.io/">gRPC&lt;/a> as the base technologies of the project.&lt;/p>
&lt;p>&lt;strong>Next steps&lt;/strong>&lt;/p>
&lt;p>We do not regard Findy Agency as a finalized product, there is still a lot to be done. However, we think it can already be used to experiment and build verified data utilizing scenarios. Our work continues with further use case implementations as well as improving the agency with selected features based on our experimentation results.&lt;/p>
&lt;p>The codes are available in &lt;a href="https://github.com/findy-network">GitHub&lt;/a> and &lt;a href="/docs">the developer documentation&lt;/a> will be improved in the near future. We look forward getting feedback from the community.&lt;/p></description></item></channel></rss>